{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Useful packages, installations, and imports for this exercise"
      ],
      "metadata": {
        "id": "MopMn34zPfMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run This Cell!\n",
        "# This just imports some packages which we will be using for this exercise\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import linear_model\n",
        "\n",
        "from scipy import sparse\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "B5Yi7PI9Pf5F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "363d6765-75e4-477a-a425-8afea85724ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Dataset"
      ],
      "metadata": {
        "id": "nIJphbRaOfFd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5CAcWz83OQ2t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6b4f9c2-af8b-43b1-c32b-e38f64a8c62d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-10 04:00:29--  https://raw.githubusercontent.com/dbamman/nlp23/main/HW1/train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1322055 (1.3M) [text/plain]\n",
            "Saving to: ‘train.txt’\n",
            "\n",
            "train.txt           100%[===================>]   1.26M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2025-02-10 04:00:29 (85.4 MB/s) - ‘train.txt’ saved [1322055/1322055]\n",
            "\n",
            "--2025-02-10 04:00:29--  https://raw.githubusercontent.com/dbamman/nlp23/main/HW1/dev.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1309909 (1.2M) [text/plain]\n",
            "Saving to: ‘dev.txt’\n",
            "\n",
            "dev.txt             100%[===================>]   1.25M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2025-02-10 04:00:29 (94.0 MB/s) - ‘dev.txt’ saved [1309909/1309909]\n",
            "\n",
            "--2025-02-10 04:00:29--  https://raw.githubusercontent.com/dbamman/nlp23/main/HW1/test.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6573426 (6.3M) [text/plain]\n",
            "Saving to: ‘test.txt’\n",
            "\n",
            "test.txt            100%[===================>]   6.27M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2025-02-10 04:00:30 (86.4 MB/s) - ‘test.txt’ saved [6573426/6573426]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# TODO\n",
        "!wget https://raw.githubusercontent.com/dbamman/nlp23/main/HW1/train.txt\n",
        "!wget https://raw.githubusercontent.com/dbamman/nlp23/main/HW1/dev.txt\n",
        "!wget https://raw.githubusercontent.com/dbamman/nlp23/main/HW1/test.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainingFile = \"train.txt\"\n",
        "evaluationFile = \"dev.txt\"\n",
        "testFile = \"test.txt\""
      ],
      "metadata": {
        "id": "YoHkssGZ26Uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Based on Info159 - Natural Language Processing HW 1 by Prof. David Bamman\n",
        "\n",
        "class FeaturizedDataLoader:\n",
        "    def __init__(self, feature_method, min_feature_count=1):\n",
        "          self.feature_vocab = {}\n",
        "          self.feature_method = feature_method\n",
        "          self.min_feature_count = min_feature_count\n",
        "          self.trainX, self.trainY, self.trainOrig = self.process(trainingFile, training=True)\n",
        "          self.devX, self.devY, self.devOrig = self.process(evaluationFile, training=False)\n",
        "          self.testX, _, self.testOrig = self.process(testFile, training=False)\n",
        "\n",
        "          # Read data from file\n",
        "    def load_data(self, filename):\n",
        "        data = []\n",
        "        with open(filename, encoding=\"utf8\") as file:\n",
        "            for line in file:\n",
        "                cols = line.split(\"\\t\")\n",
        "                idd = cols[0]\n",
        "                label = cols[1]\n",
        "                text = cols[2]\n",
        "\n",
        "                data.append((idd, label, text))\n",
        "\n",
        "        return data\n",
        "    # Featurize entire dataset\n",
        "    def featurize(self, data):\n",
        "        featurized_data = []\n",
        "        for idd, label, text in data:\n",
        "            feats = self.feature_method(text)\n",
        "            featurized_data.append((label, feats))\n",
        "        return featurized_data\n",
        "\n",
        "    # Read dataset and returned featurized representation as sparse matrix + label array\n",
        "    def process(self, dataFile, training = False):\n",
        "        original_data = self.load_data(dataFile)\n",
        "        data = self.featurize(original_data)\n",
        "\n",
        "        if training:\n",
        "            fid = 0\n",
        "            feature_doc_count = Counter()\n",
        "            for label, feats in data:\n",
        "                for feat in feats:\n",
        "                    feature_doc_count[feat]+= 1\n",
        "\n",
        "            for feat in feature_doc_count:\n",
        "                if feature_doc_count[feat] >= self.min_feature_count:\n",
        "                    self.feature_vocab[feat] = fid\n",
        "                    fid += 1\n",
        "\n",
        "        F = len(self.feature_vocab)\n",
        "        D = len(data)\n",
        "        X = sparse.dok_matrix((D, F))\n",
        "        Y = [None]*D\n",
        "        for idx, (label, feats) in enumerate(data):\n",
        "            for feat in feats:\n",
        "                if feat in self.feature_vocab:\n",
        "                    X[idx, self.feature_vocab[feat]] = feats[feat]\n",
        "            Y[idx] = label\n",
        "\n",
        "        return X, Y, original_data\n",
        "\n",
        "    def load_test(self, dataFile):\n",
        "        data = self.load_data(dataFile)\n",
        "        data = self.featurize(data)\n",
        "\n",
        "        F = len(self.feature_vocab)\n",
        "        D = len(data)\n",
        "        X = sparse.dok_matrix((D, F))\n",
        "        Y = [None]*D\n",
        "        for idx, (data_id, feats) in enumerate(data):\n",
        "            for feat in feats:\n",
        "                if feat in self.feature_vocab:\n",
        "                    X[idx, self.feature_vocab[feat]] = feats[feat]\n",
        "            Y[idx] = data_id\n",
        "\n",
        "        return X, Y"
      ],
      "metadata": {
        "id": "PFBaD450OiuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M0d6GCJ6OjDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a Simple Classifier"
      ],
      "metadata": {
        "id": "jah0wUpAOjd8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoding the Input"
      ],
      "metadata": {
        "id": "9tnAeVVIOsQW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bag of Words"
      ],
      "metadata": {
        "id": "MwGeEplUOv6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bag_of_words(text):\n",
        "    # Here the `feats` dict should contain the features -- the key should be the feature name,\n",
        "    # and the value is the feature value.  See `simple_featurize` for an example.\n",
        "\n",
        "    feats = {}\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    for word in words:\n",
        "      word=word.lower()\n",
        "\n",
        "      feats[word] = 1\n",
        "    return feats\n"
      ],
      "metadata": {
        "id": "6L6UuSHuOr1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BoW_Loader = FeaturizedDataLoader(bag_of_words)"
      ],
      "metadata": {
        "id": "EWf99Dxn_j5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's take a look\n",
        "\n",
        "# dimensions\n",
        "D, F = BoW_Loader.trainX.shape\n",
        "print(\"number of rows in train X:\", D)\n",
        "print(\"number of features:\", F)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VF8PVy-k5FrV",
        "outputId": "c8f4a833-8a08-4bf5-fd72-2e2bfec9f56a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of rows in train X: 1000\n",
            "number of features: 21078\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embeddings"
      ],
      "metadata": {
        "id": "kf--DV7qOzF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch embeddings\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove*.zip"
      ],
      "metadata": {
        "id": "aqMd0rDaO1WH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "019c12ba-9219-4d12-f4cc-89ac48f4bcd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-10 04:00:56--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2025-02-10 04:00:56--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2025-02-10 04:00:57--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.10MB/s    in 2m 50s  \n",
            "\n",
            "2025-02-10 04:03:47 (4.85 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_index = {}\n",
        "f = open('glove.6B.100d.txt')\n",
        "for line in f:\n",
        "    values = line.split(' ')\n",
        "    word = values[0] ## The first entry is the word\n",
        "    coefs = np.asarray(values[1:], dtype='float32') ## These are the vecotrs representing the embedding for the word\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZO-13ZoM7Jp",
        "outputId": "9e2b0000-52ff-4946-9155-a4182be1fcc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To make use of these embeddings, we need to ensure we have a mapping of words to indices\n",
        "# Let's define a simple word based tokenizer to work with these embeddings\n",
        "\n",
        "class WordTokenizer:\n",
        "    def __init__(self, documents=None, vocab_size=100000):\n",
        "        if documents:\n",
        "          self.id2token = Dictionary(\n",
        "              [nltk.word_tokenize(document.lower()) for document in documents],\n",
        "              prune_at=vocab_size\n",
        "          )\n",
        "          self.id2token.filter_extremes(no_below=1, no_above=1, keep_n=vocab_size)\n",
        "        else:\n",
        "          self.id2token = Dictionary(prune_at = vocab_size)\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    def __call__(self, text):\n",
        "        return self.tokenize(text)\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        tokens = nltk.word_tokenize(text.lower())\n",
        "        return self.id2token.doc2idx(tokens, unknown_word_index = self.get_vocab_size())\n",
        "\n",
        "    def get_vocab(self):\n",
        "        return self.id2token.token2id\n",
        "\n",
        "    def get_vocab_size(self):\n",
        "        return len(self.get_vocab())\n",
        "\n",
        "    def get_word_id(self, word):\n",
        "        return self.get_vocab()[word]\n",
        "\n",
        "    def get_word(self, idx):\n",
        "        return self.id2token.get(idx, None)\n",
        "\n",
        "    def add_documents(self, documents):\n",
        "        self.id2token.add_documents([nltk.word_tokenize(document.lower()) for document in documents], prune_at=self.vocab_size)\n",
        "        self.id2token.filter_extremes(no_below=1, no_above=1, keep_n=self.vocab_size)"
      ],
      "metadata": {
        "id": "VdJe-WrFOpRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's build an embedding matrix based on this tokenizer and the embeddings we loaded earlier\n",
        "\n",
        "def create_embedding_matrix(tokenizer: WordTokenizer, embeddings_index, embedding_dim=100):\n",
        "    vocab_size = tokenizer.get_vocab_size() + 1  # +1 for OOV token if needed\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim)).astype('float32')\n",
        "\n",
        "    for word in tokenizer.get_vocab():\n",
        "        if word in embeddings_index:\n",
        "            idx = tokenizer.get_word_id(word)\n",
        "            embedding_matrix[idx] = embeddings_index[word]\n",
        "\n",
        "    return embedding_matrix\n",
        "\n"
      ],
      "metadata": {
        "id": "ElGX-mqzPNtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# recall our dataloader stores the original training data in the trainOrig attribute as a list of (idd, label, text)\n",
        "\n",
        "# now we extract the text from this\n",
        "\n",
        "trainOrigDocuments = [x[2] for x in BoW_Loader.trainOrig]\n",
        "\n",
        "word_tokenizer = WordTokenizer(trainOrigDocuments, 10000)\n",
        "embedding_matrix = create_embedding_matrix(word_tokenizer, embeddings_index)"
      ],
      "metadata": {
        "id": "4KJ-L_vSrIpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzVoP1yZAbGx",
        "outputId": "e0ab5a0c-52ff-4595-d45d-827eafa94419"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10001, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(embedding_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7Eeg1wl5dAS",
        "outputId": "a7757a0e-43d9-44f9-bc23-db5eb37539a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.38472 ,  0.49351 ,  0.49096 , ...,  0.026263,  0.39052 ,\n",
              "         0.52217 ],\n",
              "       [ 0.58854 , -0.2025  ,  0.73479 , ..., -0.94475 ,  0.61802 ,\n",
              "         0.39591 ],\n",
              "       [ 0.19247 ,  0.36617 ,  0.52301 , ..., -1.2276  ,  1.1152  ,\n",
              "        -1.0234  ],\n",
              "       ...,\n",
              "       [ 0.27855 , -0.25163 ,  1.1612  , ...,  0.18109 , -0.025508,\n",
              "        -0.90374 ],\n",
              "       [-0.10752 , -0.73378 , -0.15725 , ...,  0.42029 , -0.2823  ,\n",
              "         0.45759 ],\n",
              "       [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
              "         0.      ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenizer.get_word(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0obaR54W5e3q",
        "outputId": "c662d574-93a7-47ac-8f30-366171ac3b14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_index['!']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01FK6moS57vi",
        "outputId": "e3c1d641-4d58-4a0b-d430-085f3011582a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.38472  ,  0.49351  ,  0.49096  , -1.5434   , -0.33614  ,\n",
              "        0.6222   ,  0.32265  ,  0.075331 ,  0.65591  , -0.23517  ,\n",
              "        1.2114   ,  0.06193  , -0.62004  ,  0.31371  ,  0.38948  ,\n",
              "       -0.24381  , -0.065643 ,  0.58797  , -0.86382  ,  0.63166  ,\n",
              "        0.68363  ,  0.39647  , -0.62388  , -0.25094  ,  0.92831  ,\n",
              "        1.5152   , -0.43917  ,  0.22249  ,  1.3695   , -0.53098  ,\n",
              "        0.39811  ,  0.77114  ,  0.49043  ,  0.58853  ,  0.2376   ,\n",
              "        0.3162   , -0.011962 , -0.047074 ,  0.34585  , -1.2944   ,\n",
              "        0.18597  ,  0.27002  , -0.70602  , -0.20652  , -0.25194  ,\n",
              "       -0.4868   , -0.71538  , -0.23887  , -0.041612 , -0.55488  ,\n",
              "       -0.54226  ,  0.21236  ,  0.025341 ,  0.96517  , -0.88183  ,\n",
              "       -1.8681   ,  0.32657  ,  1.1689   ,  1.1759   , -0.17393  ,\n",
              "       -0.3371   ,  0.87535  , -1.0114   , -0.6181   ,  1.008    ,\n",
              "        0.31506  ,  0.24417  ,  0.064393 ,  0.33678  ,  0.33632  ,\n",
              "        0.45975  ,  0.22813  , -0.37505  , -0.37508  ,  0.089301 ,\n",
              "        0.53862  ,  0.039714 , -0.0036392, -0.25023  , -0.18224  ,\n",
              "        0.42731  , -0.79118  , -0.29409  , -0.40693  , -1.0908   ,\n",
              "       -0.16476  , -0.41458  , -0.67899  ,  0.28319  ,  0.30937  ,\n",
              "        0.49304  , -0.067002 ,  0.50222  ,  0.73959  , -0.4735   ,\n",
              "       -0.47342  , -0.20242  ,  0.026263 ,  0.39052  ,  0.52217  ],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining a Model"
      ],
      "metadata": {
        "id": "glJl1wRnO5o2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Regression"
      ],
      "metadata": {
        "id": "2fFXI6SyO8c2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "L2_regularization_strength = 1.0"
      ],
      "metadata": {
        "id": "A6pILdP1xscL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a Logistic Regression https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
        "log_reg = LogisticRegression(C = L2_regularization_strength, max_iter=1000)\n",
        "log_reg.fit(BoW_Loader.trainX, BoW_Loader.trainY)\n",
        "\n",
        "# score it against our data\n",
        "training_accuracy = log_reg.score(BoW_Loader.trainX, BoW_Loader.trainY)\n",
        "development_accuracy = log_reg.score(BoW_Loader.devX, BoW_Loader.devY)\n",
        "\n",
        "print(\"Method: %s, Features: %s, Train accuracy: %.3f, Dev accuracy: %.3f\" % (BoW_Loader.feature_method.__name__, F, training_accuracy, development_accuracy))"
      ],
      "metadata": {
        "id": "NjFeTVyAO7t4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03afe95d-90c1-4424-fa87-71a9cc26a7ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Method: bag_of_words, Features: 21078, Train accuracy: 1.000, Dev accuracy: 0.775\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multilayer Perceptron"
      ],
      "metadata": {
        "id": "_KWLfioXPAgu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Torch Dataset and DataLoader from our data\n",
        "\n",
        "class BoW_Dataset(Dataset):\n",
        "    def __init__(self, X, Y):\n",
        "        self.X = X.toarray().astype(np.float32)\n",
        "        self.Y = Y\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.X.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.X[idx]\n",
        "        y = self.Y[idx]\n",
        "\n",
        "        # specific to our data\n",
        "        if y == 'pos':\n",
        "          y = 1\n",
        "        else:\n",
        "          y = 0\n",
        "        return x, np.array([y]).astype(np.float32)\n",
        "\n",
        "train_dataset = BoW_Dataset(BoW_Loader.trainX, BoW_Loader.trainY)\n",
        "valid_dataset = BoW_Dataset(BoW_Loader.devX, BoW_Loader.devY)\n",
        "\n",
        "# dataloader = DataLoader(train_dataset, batch_size=2)"
      ],
      "metadata": {
        "id": "VLZBxQGd0iQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Linear(F, 1),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4wNoyh4zYAT",
        "outputId": "c95c7254-0861-4d71-832b-6da8e7216b9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=21078, out_features=1, bias=True)\n",
            "  (1): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_training_loop(model, batch_size=32, n_epochs=10, lr=1e-3):\n",
        "\n",
        "    # We could write our training procedure manually and directly index the `Dataset` objects,\n",
        "    # but the `DataLoader` object conveniently creates an iterable for automatically creating random minibatches:\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Choose Adam as the optimizer\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # Use the cross entropy loss function\n",
        "    # loss_fn = nn.CrossEntropyLoss()\n",
        "    loss_fn = nn.BCELoss()\n",
        "    # store metrics\n",
        "    train_loss_history = np.zeros([n_epochs, 1])\n",
        "    valid_accuracy_history = np.zeros([n_epochs, 1])\n",
        "    valid_loss_history = np.zeros([n_epochs, 1])\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "\n",
        "        # Some layers, such as Dropout, behave differently during training\n",
        "        model.train()\n",
        "\n",
        "        train_loss = 0\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            # Erase accumulated gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            output = model(data)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = loss_fn(output, target)\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Weight update\n",
        "            optimizer.step()\n",
        "\n",
        "        train_loss_history[epoch] = train_loss / len(train_loader.dataset)\n",
        "\n",
        "        # Track loss each epoch\n",
        "        print('Train Epoch: %d  Average loss: %.4f' %\n",
        "              (epoch + 1,  train_loss_history[epoch]))\n",
        "\n",
        "        # Putting layers like Dropout into evaluation mode\n",
        "        model.eval()\n",
        "\n",
        "        valid_loss = 0\n",
        "        correct = 0\n",
        "\n",
        "        # Turning off automatic differentiation\n",
        "        with torch.no_grad():\n",
        "            for data, target in valid_loader:\n",
        "                output = model(data)\n",
        "                valid_loss += loss_fn(output, target).item()  # Sum up batch loss\n",
        "                pred = torch.round(output)\n",
        "                # pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max class score\n",
        "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "        valid_loss_history[epoch] = valid_loss / len(valid_loader.dataset)\n",
        "        valid_accuracy_history[epoch] = correct / len(valid_loader.dataset)\n",
        "\n",
        "        print('Valid set: Average loss: %.4f, Accuracy: %d/%d (%.4f)\\n' %\n",
        "              (valid_loss_history[epoch], correct, len(valid_loader.dataset),\n",
        "              100. * valid_accuracy_history[epoch]))\n",
        "\n",
        "    return model, train_loss_history, valid_loss_history, valid_accuracy_history"
      ],
      "metadata": {
        "id": "W4QBL6p80LQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_nn, train_loss_history, valid_loss_history, valid_accuracy_history = run_training_loop(model, batch_size=32, n_epochs=10, lr=1e-3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fR8ZB9n7lL_",
        "outputId": "662f2db9-4f3b-4e82-89e4-520af0215a7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-031e17bf88a2>:45: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print('Train Epoch: %d  Average loss: %.4f' %\n",
            "<ipython-input-21-031e17bf88a2>:66: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print('Valid set: Average loss: %.4f, Accuracy: %d/%d (%.4f)\\n' %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1  Average loss: 0.0214\n",
            "Valid set: Average loss: 0.0205, Accuracy: 758/1000 (75.8000)\n",
            "\n",
            "Train Epoch: 2  Average loss: 0.0173\n",
            "Valid set: Average loss: 0.0193, Accuracy: 756/1000 (75.6000)\n",
            "\n",
            "Train Epoch: 3  Average loss: 0.0145\n",
            "Valid set: Average loss: 0.0185, Accuracy: 797/1000 (79.7000)\n",
            "\n",
            "Train Epoch: 4  Average loss: 0.0126\n",
            "Valid set: Average loss: 0.0178, Accuracy: 795/1000 (79.5000)\n",
            "\n",
            "Train Epoch: 5  Average loss: 0.0110\n",
            "Valid set: Average loss: 0.0172, Accuracy: 806/1000 (80.6000)\n",
            "\n",
            "Train Epoch: 6  Average loss: 0.0097\n",
            "Valid set: Average loss: 0.0169, Accuracy: 808/1000 (80.8000)\n",
            "\n",
            "Train Epoch: 7  Average loss: 0.0087\n",
            "Valid set: Average loss: 0.0165, Accuracy: 803/1000 (80.3000)\n",
            "\n",
            "Train Epoch: 8  Average loss: 0.0078\n",
            "Valid set: Average loss: 0.0160, Accuracy: 811/1000 (81.1000)\n",
            "\n",
            "Train Epoch: 9  Average loss: 0.0071\n",
            "Valid set: Average loss: 0.0159, Accuracy: 811/1000 (81.1000)\n",
            "\n",
            "Train Epoch: 10  Average loss: 0.0065\n",
            "Valid set: Average loss: 0.0156, Accuracy: 806/1000 (80.6000)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Linear(F, 1024),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(1024, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256, 1),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSxczPXbBeyy",
        "outputId": "ea0b502b-bd69-4571-d582-229637234a1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=21078, out_features=1024, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "  (3): ReLU()\n",
            "  (4): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (5): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(word_tokenizer.get_vocab())\n",
        "word_tokenizer.vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OX-DC9wb_Vv9",
        "outputId": "f1b0b964-99c7-43d6-d1c9-911ca7825924"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mlp, train_loss_history, valid_loss_history, valid_accuracy_history = run_training_loop(model, batch_size=32, n_epochs=10, lr=1e-3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJxE1RDhAcXl",
        "outputId": "0b306d7e-4906-42b8-f557-2124e1f1baee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-031e17bf88a2>:45: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print('Train Epoch: %d  Average loss: %.4f' %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1  Average loss: 0.0174\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-031e17bf88a2>:66: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print('Valid set: Average loss: %.4f, Accuracy: %d/%d (%.4f)\\n' %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valid set: Average loss: 0.0150, Accuracy: 809/1000 (80.9000)\n",
            "\n",
            "Train Epoch: 2  Average loss: 0.0011\n",
            "Valid set: Average loss: 0.0216, Accuracy: 795/1000 (79.5000)\n",
            "\n",
            "Train Epoch: 3  Average loss: 0.0001\n",
            "Valid set: Average loss: 0.0304, Accuracy: 761/1000 (76.1000)\n",
            "\n",
            "Train Epoch: 4  Average loss: 0.0000\n",
            "Valid set: Average loss: 0.0262, Accuracy: 790/1000 (79.0000)\n",
            "\n",
            "Train Epoch: 5  Average loss: 0.0000\n",
            "Valid set: Average loss: 0.0272, Accuracy: 787/1000 (78.7000)\n",
            "\n",
            "Train Epoch: 6  Average loss: 0.0000\n",
            "Valid set: Average loss: 0.0293, Accuracy: 788/1000 (78.8000)\n",
            "\n",
            "Train Epoch: 7  Average loss: 0.0000\n",
            "Valid set: Average loss: 0.0323, Accuracy: 787/1000 (78.7000)\n",
            "\n",
            "Train Epoch: 8  Average loss: 0.0000\n",
            "Valid set: Average loss: 0.0357, Accuracy: 786/1000 (78.6000)\n",
            "\n",
            "Train Epoch: 9  Average loss: 0.0000\n",
            "Valid set: Average loss: 0.0353, Accuracy: 786/1000 (78.6000)\n",
            "\n",
            "Train Epoch: 10  Average loss: 0.0000\n",
            "Valid set: Average loss: 0.0367, Accuracy: 787/1000 (78.7000)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convolutional Neural Network"
      ],
      "metadata": {
        "id": "1adjh_i1PFPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's define a dataset using our word tokenizer\n",
        "\n",
        "\n",
        "class Tokenized_Dataset(Dataset):\n",
        "    def __init__(self, X, Y, tokenizer, max_seq_len=500):\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.X[idx]\n",
        "        y = self.Y[idx]\n",
        "\n",
        "        # use the tokenizer\n",
        "\n",
        "        x = self.tokenizer(x)\n",
        "\n",
        "        # Pad or truncate to max_seq_len\n",
        "        if len(x) < self.max_seq_len:\n",
        "            x = x + [self.tokenizer.get_vocab_size()] * (self.max_seq_len - len(x))\n",
        "        else:\n",
        "            x = x[:self.max_seq_len]\n",
        "        x = np.array(x)\n",
        "        # specific to our data\n",
        "        if y == 'pos':\n",
        "          y = 1\n",
        "        else:\n",
        "          y = 0\n",
        "        return x, np.array([y]).astype(np.float32)\n",
        "\n",
        "trainOrigDocuments = [x[2] for x in BoW_Loader.trainOrig]\n",
        "validOrigDocuments = [x[2] for x in BoW_Loader.devOrig]\n",
        "\n",
        "train_dataset = Tokenized_Dataset(trainOrigDocuments, BoW_Loader.trainY, word_tokenizer, max_seq_len = 500)\n",
        "valid_dataset = Tokenized_Dataset(validOrigDocuments, BoW_Loader.devY, word_tokenizer, max_seq_len = 500)"
      ],
      "metadata": {
        "id": "T-WoDFe1EfSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's try to use the embeddings we loaded earlier now\n",
        "model = nn.Sequential(\n",
        "    nn.Embedding.from_pretrained(torch.from_numpy(embedding_matrix)) # by default this is frozen ie will not be updated during training\n",
        ")\n",
        "print(model)"
      ],
      "metadata": {
        "id": "1RD3FY82PHLI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47ae4a0a-0349-41ab-80c5-90933b788618"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Embedding(10001, 100)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's mimic what will happen with our embeddings\n",
        "max_seq_len = 500\n",
        "\n",
        "x = BoW_Loader.trainOrig[0][2]\n",
        "x = word_tokenizer(x)\n",
        "padding_token = word_tokenizer.get_vocab_size()\n",
        "\n",
        "# Pad or truncate to max_seq_len\n",
        "if len(x) < max_seq_len:\n",
        "    x = x + [word_tokenizer.get_vocab_size()] * (max_seq_len - len(x))\n",
        "else:\n",
        "    x = x[:max_seq_len]\n",
        "print(\"padding and unknown token is:\", padding_token)\n",
        "print(\"Tokenized Sequence is:\")\n",
        "print(np.array(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Kf1HoMS9d5W",
        "outputId": "1b593812-c33f-47e9-c756-30f9da5649fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "padding and unknown token is: 10000\n",
            "Tokenized Sequence is:\n",
            "[   50    94     4    75     1   126   156   130    89   105     8     6\n",
            "     3   133    19    63     4    52   153    68   130   127    78    16\n",
            "   110   129    19   113   129   155    55    90   152    88    26    17\n",
            "     9     7     3   130    84    69    92    51    13   101     4   122\n",
            "    66   148   155    55   118    16   126   159    42   130    85   140\n",
            "    11    69    67    25   130    84    69   147   111     5    68    15\n",
            "    96   141   131     2    77   130    98    64   149   139     3     4\n",
            "   160    30    61   130   116    57   137   130   108    21   138     5\n",
            "    64    80   129     5   130    84    40    92    71   160    27   156\n",
            "    86     4    91    45     8    70    76   160   117   144   150    69\n",
            "    58    69    10    74    93   119   129    54    15    22   129    69\n",
            "    49    15   145    46     5   130   120    72    93    22    53    29\n",
            "   125    73    68    47   154   121     5    92    51    13   102   158\n",
            "    62   115    70   139    48    93    33     4    20   130    34   157\n",
            "    69    14     5   135   129    23   132    18    93    10    84   123\n",
            "    97   130   127   155    24   145    38     5    70     1   130    72\n",
            "    93   127   129   160    60   139    81   143   130    79    68   161\n",
            "    82    12     5     2    87   146    93    70    69   106    37    35\n",
            "     4    28   107   112    95     0     3    65   160    39    56    31\n",
            "   130    84     4   133    69    98   147    35   100     8   151   130\n",
            "   140    36    19   128   139    43    99    97   132    32   104     5\n",
            "    15   142   124    44   129   114   109   160    68   130    83     5\n",
            "    59    94   139   130   103   129   136    93    41   134     5 10000\n",
            " 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
            " 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
            " 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
            " 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
            " 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
            " 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
            " 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
            " 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
            " 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
            " 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
            " 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
            " 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
            " 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
            " 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
            " 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
            " 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
            " 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
            " 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
            " 10000 10000 10000 10000 10000 10000 10000 10000]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model(torch.tensor(np.array([x])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyApLMs5_GGj",
        "outputId": "ab7c6516-0e0d-45e2-91fe-19a0394e728a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.0201,  0.0375,  0.3536,  ...,  0.0626,  0.2839, -0.3163],\n",
              "         [-0.2946, -0.2074,  0.4911,  ...,  0.5775,  0.5217, -0.0270],\n",
              "         [-0.1077,  0.1105,  0.5981,  ..., -0.8316,  0.4529,  0.0826],\n",
              "         ...,\n",
              "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model(torch.tensor(np.array([x]))).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Yq3MqYUDZ5Z",
        "outputId": "33b47a83-9ff7-40ad-d6ac-a7af490a167c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 500, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# notice that this is currently (1, seq_len, embedding_dim) when we use this in our dataloader we'll have (batch_size, seq_len, embedding_dim) but the convolutional layers we use expect (batch_size, embedding_dim, seq_len)\n",
        "\n",
        "# Let's look at how we can create a custom layer for use in Pytorch that will change this for us\n",
        "\n",
        "class Transpose(nn.Module):\n",
        "    def __init__(self, dim0, dim1):\n",
        "        super().__init__()\n",
        "        self.dim0 = dim0\n",
        "        self.dim1 = dim1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.transpose(x,self.dim0, self.dim1)\n",
        "        return x"
      ],
      "metadata": {
        "id": "BKdB8geXGBPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a utility to help us calculate the dims of the hidden layer following our convolution layer\n",
        "\n",
        "def calc_linear_dim(max_seq_len, num_filters, kernel_size, stride, max_pool_size):\n",
        "  \"\"\"\n",
        "  Calculates the in size of a linear layer following convolutions and max pooling\n",
        "\n",
        "  Assumes no padding is used\n",
        "  \"\"\"\n",
        "  return ((max_seq_len-kernel_size)//stride + 1)//max_pool_size*num_filters"
      ],
      "metadata": {
        "id": "b1m2K4isDh5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We'll use this in combination with a CNN since now we have sequential data\n",
        "# let's try to use the embeddings we loaded earlier now\n",
        "\n",
        "# recall max_seq_len is 500 right now\n",
        "\n",
        "max_seq_len = 500\n",
        "num_filters = 64\n",
        "kernel_size = 5\n",
        "stride = 1\n",
        "max_pool_size = 20\n",
        "\n",
        "linear_size = calc_linear_dim(max_seq_len, num_filters, kernel_size, stride, max_pool_size)\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Embedding.from_pretrained(torch.from_numpy(embedding_matrix)), # by default this is frozen ie will not be updated during training\n",
        "    Transpose(1, 2),\n",
        "    nn.Conv1d(100, num_filters, kernel_size),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool1d(max_pool_size),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(linear_size, 1),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce2X0hOfCgYY",
        "outputId": "d8136de1-1a16-4165-e40c-286393320c52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Embedding(10001, 100)\n",
            "  (1): Transpose()\n",
            "  (2): Conv1d(100, 64, kernel_size=(5,), stride=(1,))\n",
            "  (3): ReLU()\n",
            "  (4): MaxPool1d(kernel_size=20, stride=20, padding=0, dilation=1, ceil_mode=False)\n",
            "  (5): Flatten(start_dim=1, end_dim=-1)\n",
            "  (6): Linear(in_features=1536, out_features=1, bias=True)\n",
            "  (7): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn, train_loss_history, valid_loss_history, valid_accuracy_history = run_training_loop(model, batch_size=32, n_epochs=15, lr=5e-3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFY7SiYLlnGo",
        "outputId": "6e3c7c81-02b2-4bec-f986-47334dc09129"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-031e17bf88a2>:45: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print('Train Epoch: %d  Average loss: %.4f' %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1  Average loss: 0.0230\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-031e17bf88a2>:66: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print('Valid set: Average loss: %.4f, Accuracy: %d/%d (%.4f)\\n' %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valid set: Average loss: 0.0222, Accuracy: 504/1000 (50.4000)\n",
            "\n",
            "Train Epoch: 2  Average loss: 0.0219\n",
            "Valid set: Average loss: 0.0220, Accuracy: 545/1000 (54.5000)\n",
            "\n",
            "Train Epoch: 3  Average loss: 0.0208\n",
            "Valid set: Average loss: 0.0211, Accuracy: 610/1000 (61.0000)\n",
            "\n",
            "Train Epoch: 4  Average loss: 0.0195\n",
            "Valid set: Average loss: 0.0201, Accuracy: 663/1000 (66.3000)\n",
            "\n",
            "Train Epoch: 5  Average loss: 0.0166\n",
            "Valid set: Average loss: 0.0188, Accuracy: 684/1000 (68.4000)\n",
            "\n",
            "Train Epoch: 6  Average loss: 0.0134\n",
            "Valid set: Average loss: 0.0185, Accuracy: 701/1000 (70.1000)\n",
            "\n",
            "Train Epoch: 7  Average loss: 0.0096\n",
            "Valid set: Average loss: 0.0195, Accuracy: 715/1000 (71.5000)\n",
            "\n",
            "Train Epoch: 8  Average loss: 0.0063\n",
            "Valid set: Average loss: 0.0238, Accuracy: 700/1000 (70.0000)\n",
            "\n",
            "Train Epoch: 9  Average loss: 0.0032\n",
            "Valid set: Average loss: 0.0211, Accuracy: 713/1000 (71.3000)\n",
            "\n",
            "Train Epoch: 10  Average loss: 0.0017\n",
            "Valid set: Average loss: 0.0222, Accuracy: 719/1000 (71.9000)\n",
            "\n",
            "Train Epoch: 11  Average loss: 0.0008\n",
            "Valid set: Average loss: 0.0249, Accuracy: 727/1000 (72.7000)\n",
            "\n",
            "Train Epoch: 12  Average loss: 0.0005\n",
            "Valid set: Average loss: 0.0247, Accuracy: 728/1000 (72.8000)\n",
            "\n",
            "Train Epoch: 13  Average loss: 0.0003\n",
            "Valid set: Average loss: 0.0254, Accuracy: 710/1000 (71.0000)\n",
            "\n",
            "Train Epoch: 14  Average loss: 0.0002\n",
            "Valid set: Average loss: 0.0254, Accuracy: 720/1000 (72.0000)\n",
            "\n",
            "Train Epoch: 15  Average loss: 0.0001\n",
            "Valid set: Average loss: 0.0270, Accuracy: 713/1000 (71.3000)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# recall max_seq_len is 500 right now\n",
        "num_filters = 64\n",
        "kernel_size = 5\n",
        "stride = 1\n",
        "max_pool_size = 20\n",
        "\n",
        "linear_size = calc_linear_dim(max_seq_len, num_filters, kernel_size, stride, max_pool_size)\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1]), # let's fit our own custom embeddings on this data of the same shape\n",
        "    Transpose(1, 2),\n",
        "    nn.Conv1d(embedding_matrix.shape[1], num_filters, kernel_size),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool1d(max_pool_size),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(linear_size, 1),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJ-YOHinrARW",
        "outputId": "f12b5fc4-9c38-4d69-9fd4-f886dc350ad0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Embedding(10001, 100)\n",
            "  (1): Transpose()\n",
            "  (2): Conv1d(100, 64, kernel_size=(5,), stride=(1,))\n",
            "  (3): ReLU()\n",
            "  (4): MaxPool1d(kernel_size=20, stride=20, padding=0, dilation=1, ceil_mode=False)\n",
            "  (5): Flatten(start_dim=1, end_dim=-1)\n",
            "  (6): Linear(in_features=1536, out_features=1, bias=True)\n",
            "  (7): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn, train_loss_history, valid_loss_history, valid_accuracy_history = run_training_loop(model, batch_size=32, n_epochs=15, lr=5e-3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxokN4tKF_oU",
        "outputId": "19dce366-6cc6-4ed8-c346-563bd6b75feb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-031e17bf88a2>:45: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print('Train Epoch: %d  Average loss: %.4f' %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1  Average loss: 0.0324\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-031e17bf88a2>:66: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print('Valid set: Average loss: %.4f, Accuracy: %d/%d (%.4f)\\n' %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valid set: Average loss: 0.0225, Accuracy: 541/1000 (54.1000)\n",
            "\n",
            "Train Epoch: 2  Average loss: 0.0172\n",
            "Valid set: Average loss: 0.0232, Accuracy: 532/1000 (53.2000)\n",
            "\n",
            "Train Epoch: 3  Average loss: 0.0080\n",
            "Valid set: Average loss: 0.0236, Accuracy: 556/1000 (55.6000)\n",
            "\n",
            "Train Epoch: 4  Average loss: 0.0018\n",
            "Valid set: Average loss: 0.0250, Accuracy: 554/1000 (55.4000)\n",
            "\n",
            "Train Epoch: 5  Average loss: 0.0005\n",
            "Valid set: Average loss: 0.0258, Accuracy: 548/1000 (54.8000)\n",
            "\n",
            "Train Epoch: 6  Average loss: 0.0002\n",
            "Valid set: Average loss: 0.0259, Accuracy: 558/1000 (55.8000)\n",
            "\n",
            "Train Epoch: 7  Average loss: 0.0001\n",
            "Valid set: Average loss: 0.0266, Accuracy: 551/1000 (55.1000)\n",
            "\n",
            "Train Epoch: 8  Average loss: 0.0001\n",
            "Valid set: Average loss: 0.0272, Accuracy: 554/1000 (55.4000)\n",
            "\n",
            "Train Epoch: 9  Average loss: 0.0001\n",
            "Valid set: Average loss: 0.0271, Accuracy: 549/1000 (54.9000)\n",
            "\n",
            "Train Epoch: 10  Average loss: 0.0001\n",
            "Valid set: Average loss: 0.0280, Accuracy: 547/1000 (54.7000)\n",
            "\n",
            "Train Epoch: 11  Average loss: 0.0001\n",
            "Valid set: Average loss: 0.0279, Accuracy: 555/1000 (55.5000)\n",
            "\n",
            "Train Epoch: 12  Average loss: 0.0000\n",
            "Valid set: Average loss: 0.0280, Accuracy: 551/1000 (55.1000)\n",
            "\n",
            "Train Epoch: 13  Average loss: 0.0000\n",
            "Valid set: Average loss: 0.0284, Accuracy: 553/1000 (55.3000)\n",
            "\n",
            "Train Epoch: 14  Average loss: 0.0000\n",
            "Valid set: Average loss: 0.0283, Accuracy: 553/1000 (55.3000)\n",
            "\n",
            "Train Epoch: 15  Average loss: 0.0000\n",
            "Valid set: Average loss: 0.0291, Accuracy: 552/1000 (55.2000)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# recall max_seq_len is 500 right now\n",
        "num_filters = 8\n",
        "kernel_size = 5\n",
        "stride = 1\n",
        "max_pool_size = 20\n",
        "embedding_dim = 32\n",
        "\n",
        "\n",
        "linear_size = calc_linear_dim(max_seq_len, num_filters, kernel_size, stride, max_pool_size)\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Embedding(embedding_matrix.shape[0], embedding_dim), # let's fit our own custom embeddings with our choice of dimensions\n",
        "    Transpose(1, 2),\n",
        "    nn.Conv1d(embedding_dim, num_filters, kernel_size),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool1d(max_pool_size),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(linear_size, 1),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DqXP7NiGM-O",
        "outputId": "1ba634ab-f6c2-4584-ef50-7f59cb1b4034"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Embedding(10001, 32)\n",
            "  (1): Transpose()\n",
            "  (2): Conv1d(32, 8, kernel_size=(5,), stride=(1,))\n",
            "  (3): ReLU()\n",
            "  (4): MaxPool1d(kernel_size=20, stride=20, padding=0, dilation=1, ceil_mode=False)\n",
            "  (5): Flatten(start_dim=1, end_dim=-1)\n",
            "  (6): Linear(in_features=192, out_features=1, bias=True)\n",
            "  (7): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn, train_loss_history, valid_loss_history, valid_accuracy_history = run_training_loop(model, batch_size=32, n_epochs=15, lr=5e-3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtxdDJTMGhzn",
        "outputId": "afe7a1ab-0581-4d8c-f0a7-7ef2261501b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-031e17bf88a2>:45: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print('Train Epoch: %d  Average loss: %.4f' %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1  Average loss: 0.0226\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-031e17bf88a2>:66: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print('Valid set: Average loss: %.4f, Accuracy: %d/%d (%.4f)\\n' %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valid set: Average loss: 0.0222, Accuracy: 519/1000 (51.9000)\n",
            "\n",
            "Train Epoch: 2  Average loss: 0.0202\n",
            "Valid set: Average loss: 0.0224, Accuracy: 529/1000 (52.9000)\n",
            "\n",
            "Train Epoch: 3  Average loss: 0.0167\n",
            "Valid set: Average loss: 0.0220, Accuracy: 553/1000 (55.3000)\n",
            "\n",
            "Train Epoch: 4  Average loss: 0.0114\n",
            "Valid set: Average loss: 0.0233, Accuracy: 568/1000 (56.8000)\n",
            "\n",
            "Train Epoch: 5  Average loss: 0.0057\n",
            "Valid set: Average loss: 0.0239, Accuracy: 580/1000 (58.0000)\n",
            "\n",
            "Train Epoch: 6  Average loss: 0.0022\n",
            "Valid set: Average loss: 0.0249, Accuracy: 573/1000 (57.3000)\n",
            "\n",
            "Train Epoch: 7  Average loss: 0.0010\n",
            "Valid set: Average loss: 0.0260, Accuracy: 579/1000 (57.9000)\n",
            "\n",
            "Train Epoch: 8  Average loss: 0.0006\n",
            "Valid set: Average loss: 0.0266, Accuracy: 573/1000 (57.3000)\n",
            "\n",
            "Train Epoch: 9  Average loss: 0.0004\n",
            "Valid set: Average loss: 0.0273, Accuracy: 580/1000 (58.0000)\n",
            "\n",
            "Train Epoch: 10  Average loss: 0.0003\n",
            "Valid set: Average loss: 0.0281, Accuracy: 576/1000 (57.6000)\n",
            "\n",
            "Train Epoch: 11  Average loss: 0.0002\n",
            "Valid set: Average loss: 0.0282, Accuracy: 580/1000 (58.0000)\n",
            "\n",
            "Train Epoch: 12  Average loss: 0.0001\n",
            "Valid set: Average loss: 0.0284, Accuracy: 575/1000 (57.5000)\n",
            "\n",
            "Train Epoch: 13  Average loss: 0.0001\n",
            "Valid set: Average loss: 0.0291, Accuracy: 579/1000 (57.9000)\n",
            "\n",
            "Train Epoch: 14  Average loss: 0.0001\n",
            "Valid set: Average loss: 0.0289, Accuracy: 577/1000 (57.7000)\n",
            "\n",
            "Train Epoch: 15  Average loss: 0.0001\n",
            "Valid set: Average loss: 0.0308, Accuracy: 578/1000 (57.8000)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}