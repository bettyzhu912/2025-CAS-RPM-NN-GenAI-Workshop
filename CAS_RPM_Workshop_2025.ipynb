{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bettyzhu912/2025-CAS-RPM-NN-GenAI-Workshop/blob/working-branch/CAS_RPM_Workshop_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Useful packages, installations, and imports for this exercise"
      ],
      "metadata": {
        "id": "MopMn34zPfMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "GITHUB_TOKEN = userdata.get('GITHUB_SECRET')\n",
        "\n",
        "# os.environ['GITHUB_TOKEN'] = GITHUB_TOKEN\n"
      ],
      "metadata": {
        "id": "aqBhehVOLqDB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone --branch working-branch https://{GITHUB_TOKEN}@github.com/bettyzhu912/2025-CAS-RPM-NN-GenAI-Workshop.git"
      ],
      "metadata": {
        "id": "pO33PQQgMTWE",
        "outputId": "2ef88f81-a861-45c2-d6c7-7fcf560f8234",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '2025-CAS-RPM-NN-GenAI-Workshop'...\n",
            "remote: Enumerating objects: 29, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 29 (delta 0), reused 1 (delta 0), pack-reused 25 (from 3)\u001b[K\n",
            "Receiving objects: 100% (29/29), 42.57 MiB | 25.57 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run This Cell!\n",
        "# This just imports some packages which we will be using for this exercise\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import linear_model\n",
        "\n",
        "from scipy import sparse\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "B5Yi7PI9Pf5F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d78e984d-a53d-49cd-fce8-3c278adea101"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Dataset"
      ],
      "metadata": {
        "id": "nIJphbRaOfFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/2025-CAS-RPM-NN-GenAI-Workshop/prompt.csv')"
      ],
      "metadata": {
        "id": "fo1L3ZzIORIX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(10)"
      ],
      "metadata": {
        "id": "GvBTMS-POeVM",
        "outputId": "fcd4ef6f-49fc-4af9-ff5a-bc7ead5001d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                         personality  \\\n",
              "0  You are a claims adjuster at a California home...   \n",
              "1  You are a claims adjuster at a California home...   \n",
              "2  You are a claims adjuster at a California home...   \n",
              "3  You are a claims adjuster at a California home...   \n",
              "4  You are a claims adjuster at a California home...   \n",
              "5  You are a claims adjuster at a California home...   \n",
              "6  You are a claims adjuster at a California home...   \n",
              "7  You are a claims adjuster at a California home...   \n",
              "8  You are a claims adjuster at a California home...   \n",
              "9  You are a claims adjuster at a California home...   \n",
              "\n",
              "                                              prompt  \\\n",
              "0  You just received a call about a wildfire. The...   \n",
              "1  You just received a call about a wildfire. nea...   \n",
              "2  You just received a call about a wildfire. The...   \n",
              "3  You just received a call about a wildfire. The...   \n",
              "4  You just received a call about a wildfire. The...   \n",
              "5  You just received a call about a wildfire. The...   \n",
              "6  You just received a call about a wildfire. The...   \n",
              "7  You just received a call about a wildfire. The...   \n",
              "8  You just received a call about a wildfire. The...   \n",
              "9  You just received a call about a wildfire. The...   \n",
              "\n",
              "                                            response  \\\n",
              "0  The policyholder reported damage due to a near...   \n",
              "1  The policyholder reported potential fire damag...   \n",
              "2  The policyholder reported damage from a recent...   \n",
              "3  The policyholder reported damage from the rece...   \n",
              "4  The claimant reported damage to their property...   \n",
              "5  The policyholder reported damage from a recent...   \n",
              "6  The policyholder reported a wildfire incident ...   \n",
              "7  Claimant reported damage from a recent wildfir...   \n",
              "8  The claimant reported damage to the main struc...   \n",
              "9  The policyholder reported a wildfire event tha...   \n",
              "\n",
              "                                           response2      sus     cert  \\\n",
              "0  Further investigation is necessary to assess t...  average     high   \n",
              "1  The policyholder indicated that they have disc...     high     high   \n",
              "2  Further investigation is required to assess th...     high     high   \n",
              "3  The homeowner confirmed that the charred music...  average     high   \n",
              "4  The claim will require further investigation t...  average     high   \n",
              "5  Further investigation is required to fully ass...     high  average   \n",
              "6  The policyholder confirmed that there are no s...  average     high   \n",
              "7  After a thorough investigation into the wildfi...  average     high   \n",
              "8  The claimant reiterated the engineer's recomme...  average     high   \n",
              "9  During the follow-up, I encountered a delay in...     high  average   \n",
              "\n",
              "   fraud_prob  repudiation  \n",
              "0       0.433        False  \n",
              "1       0.651        False  \n",
              "2       0.721        False  \n",
              "3       0.452        False  \n",
              "4       0.504        False  \n",
              "5       0.627        False  \n",
              "6       0.456        False  \n",
              "7       0.568        False  \n",
              "8       0.516        False  \n",
              "9       0.499        False  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ce9719a5-2051-418e-9d98-2c51f19c36b9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>personality</th>\n",
              "      <th>prompt</th>\n",
              "      <th>response</th>\n",
              "      <th>response2</th>\n",
              "      <th>sus</th>\n",
              "      <th>cert</th>\n",
              "      <th>fraud_prob</th>\n",
              "      <th>repudiation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>You are a claims adjuster at a California home...</td>\n",
              "      <td>You just received a call about a wildfire. The...</td>\n",
              "      <td>The policyholder reported damage due to a near...</td>\n",
              "      <td>Further investigation is necessary to assess t...</td>\n",
              "      <td>average</td>\n",
              "      <td>high</td>\n",
              "      <td>0.433</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>You are a claims adjuster at a California home...</td>\n",
              "      <td>You just received a call about a wildfire. nea...</td>\n",
              "      <td>The policyholder reported potential fire damag...</td>\n",
              "      <td>The policyholder indicated that they have disc...</td>\n",
              "      <td>high</td>\n",
              "      <td>high</td>\n",
              "      <td>0.651</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>You are a claims adjuster at a California home...</td>\n",
              "      <td>You just received a call about a wildfire. The...</td>\n",
              "      <td>The policyholder reported damage from a recent...</td>\n",
              "      <td>Further investigation is required to assess th...</td>\n",
              "      <td>high</td>\n",
              "      <td>high</td>\n",
              "      <td>0.721</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>You are a claims adjuster at a California home...</td>\n",
              "      <td>You just received a call about a wildfire. The...</td>\n",
              "      <td>The policyholder reported damage from the rece...</td>\n",
              "      <td>The homeowner confirmed that the charred music...</td>\n",
              "      <td>average</td>\n",
              "      <td>high</td>\n",
              "      <td>0.452</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>You are a claims adjuster at a California home...</td>\n",
              "      <td>You just received a call about a wildfire. The...</td>\n",
              "      <td>The claimant reported damage to their property...</td>\n",
              "      <td>The claim will require further investigation t...</td>\n",
              "      <td>average</td>\n",
              "      <td>high</td>\n",
              "      <td>0.504</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>You are a claims adjuster at a California home...</td>\n",
              "      <td>You just received a call about a wildfire. The...</td>\n",
              "      <td>The policyholder reported damage from a recent...</td>\n",
              "      <td>Further investigation is required to fully ass...</td>\n",
              "      <td>high</td>\n",
              "      <td>average</td>\n",
              "      <td>0.627</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>You are a claims adjuster at a California home...</td>\n",
              "      <td>You just received a call about a wildfire. The...</td>\n",
              "      <td>The policyholder reported a wildfire incident ...</td>\n",
              "      <td>The policyholder confirmed that there are no s...</td>\n",
              "      <td>average</td>\n",
              "      <td>high</td>\n",
              "      <td>0.456</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>You are a claims adjuster at a California home...</td>\n",
              "      <td>You just received a call about a wildfire. The...</td>\n",
              "      <td>Claimant reported damage from a recent wildfir...</td>\n",
              "      <td>After a thorough investigation into the wildfi...</td>\n",
              "      <td>average</td>\n",
              "      <td>high</td>\n",
              "      <td>0.568</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>You are a claims adjuster at a California home...</td>\n",
              "      <td>You just received a call about a wildfire. The...</td>\n",
              "      <td>The claimant reported damage to the main struc...</td>\n",
              "      <td>The claimant reiterated the engineer's recomme...</td>\n",
              "      <td>average</td>\n",
              "      <td>high</td>\n",
              "      <td>0.516</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>You are a claims adjuster at a California home...</td>\n",
              "      <td>You just received a call about a wildfire. The...</td>\n",
              "      <td>The policyholder reported a wildfire event tha...</td>\n",
              "      <td>During the follow-up, I encountered a delay in...</td>\n",
              "      <td>high</td>\n",
              "      <td>average</td>\n",
              "      <td>0.499</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ce9719a5-2051-418e-9d98-2c51f19c36b9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ce9719a5-2051-418e-9d98-2c51f19c36b9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ce9719a5-2051-418e-9d98-2c51f19c36b9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-27bda107-1703-49f4-85b0-c08d85ec187f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-27bda107-1703-49f4-85b0-c08d85ec187f')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-27bda107-1703-49f4-85b0-c08d85ec187f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5757,\n  \"fields\": [\n    {\n      \"column\": \"personality\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1960,\n        \"samples\": [\n          \"You are a claims adjuster at a California homeowners insurance company. Your personality is primarily Brief with a secondary trait of Busy. Provide a claim note of 1 sentences in paragraph form. Do not put 'insert date' or anything like that - do not reference the date or year. You do not need to anything like received a call or during a call - it is implied as this note goes into the field for the initial call notes. Simply describe the notes from the call. Do not mention the policy limits - use them to inform the nature of the call and the nuance of the claim. Do not make statements about anything outside the claim that an LLM would say such as 'this reflects the need for careful consideration when handling fire risks' as a claims person would not say that. Remember, this is just the first call so all the details may not be known. The notes are quick systems notes and should not be conversational.You wrote up a claim after talking to the homeowner himself.\",\n          \"You are a claims adjuster at a California homeowners insurance company. Your personality is primarily Verbose with a secondary trait of Busy. Provide a claim note of 2 sentences in paragraph form. Do not put 'insert date' or anything like that - do not reference the date or year. You do not need to anything like received a call or during a call - it is implied as this note goes into the field for the initial call notes. Simply describe the notes from the call. Do not mention the policy limits - use them to inform the nature of the call and the nuance of the claim. Do not make statements about anything outside the claim that an LLM would say such as 'this reflects the need for careful consideration when handling fire risks' as a claims person would not say that. Remember, this is just the first call so all the details may not be known. The notes are quick systems notes and should not be conversational.You took the official statement from the primary policyholder.\",\n          \"You are a claims adjuster at a California homeowners insurance company. Your personality is primarily Verbose with a secondary trait of Curious. Provide a claim note of 2 sentences in paragraph form. Do not put 'insert date' or anything like that - do not reference the date or year. You do not need to anything like received a call or during a call - it is implied as this note goes into the field for the initial call notes. Simply describe the notes from the call. Do not mention the policy limits - use them to inform the nature of the call and the nuance of the claim. Do not make statements about anything outside the claim that an LLM would say such as 'this reflects the need for careful consideration when handling fire risks' as a claims person would not say that. Remember, this is just the first call so all the details may not be known. The notes are quick systems notes and should not be conversational.You clarified questions while speaking with the homeowner herself.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"prompt\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5757,\n        \"samples\": [\n          \"You just received a call about a wildfire. The policyholder has a total limit for the main structure of 1034000 dollars, and contents coverage of 642000 dollars. Draft the first claims note you write, reflecting your personality traits. Provide a summary of the details discussed with the caller, acknowledging the unique aspects of this claim. Meld at least one nuance into your story, ensuring it influences the plot significantly: 'Sealed windows prevented fire damage but not smoke intrusion', 'Greenhouse glass shattered from high temperatures', 'Safes and metal cabinets partially melted in debris'. These nuances describe specific property/claim characteristics. Focus on creating this initial claims note based on your company's expectations, aligned with your personality and the nature of a wildfire fire incident. Use a professional but approachable tone, as if you're noting the claim details in the insurer's system. Remember this is just the first call and not all the details are known yet. The notes should be realistic and not goofy.\",\n          \"You just received a call about a wildfire. The policyholder has coverage for the main structure and contents. Draft the first claims note you write, reflecting your personality traits. Provide a summary of the details discussed with the caller, acknowledging the unique aspects of this claim. Be sure to incorporate or be inspired by at least one nuance from the list below: 'Electronics covered in ash, some devices no longer functional', 'Fine soot coating electronics and kitchen appliances', 'Sealed windows prevented fire damage but not smoke intrusion'. These nuances describe specific property/claim characteristics. Focus on creating this initial claims note based on your company's expectations, aligned with your personality and the nature of a wildfire fire incident. Use a professional but approachable tone, as if you're noting the claim details in the insurer's system. Remember this is just the first call and not all the details are known yet. The notes should be realistic and not goofy.\",\n          \"You just received a call about a wildfire. The policyholder has a total limit for the main structure of  889000 dollars, and contents coverage of 510000 dollars. Draft the first claims note you write, reflecting your personality traits. Provide a summary of the details discussed with the caller, acknowledging the unique aspects of this claim. Integrate one or two nuances from below to create depth in your story\\u2019s context: 'Scheduling contractor estimates with extended wait times', 'Outdoor lighting fixtures warped by heat blasts', 'Anxiety over unclear timeline for returning home'. These nuances describe specific property/claim characteristics. Focus on creating this initial claims note based on your company's expectations, aligned with your personality and the nature of a wildfire fire incident. Use a professional but approachable tone, as if you're noting the claim details in the insurer's system. Remember this is just the first call and not all the details are known yet. The notes should be realistic and not goofy.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"response\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5757,\n        \"samples\": [\n          \"The policyholder reported a wildfire incident affecting their property, noting that the sealed windows effectively prevented fire damage but allowed smoke intrusion into the home. Additionally, they mentioned that greenhouse glass shattered due to high temperatures, and metal cabinets and safes experienced partial melting from the debris. Further investigation will be necessary to assess the full extent of the damage to both the main structure and contents, as the unique characteristics of the loss may influence the claims process.\",\n          \"The policyholder reported damage from a nearby wildfire, noting that while the sealed windows prevented direct fire damage, smoke intrusion has affected the interior of the home. They mentioned that various electronics are covered in ash and some devices are no longer functional due to fine soot coating, particularly in the kitchen area. The policyholder is assessing the extent of the damage and will provide additional details as they inspect the property further.\",\n          \"The policyholder reported damage due to a recent wildfire, affecting both the main structure and outdoor areas. They expressed anxiety over the unclear timeline for returning home, as they are currently displaced and concerned about the extent of the damage. Additionally, they noted that outdoor lighting fixtures have warped due to heat blasts, which may require specialized replacement. The policyholder is aware of potential extended wait times for contractor estimates, which could complicate the repair process. Further details will be gathered as the claim progresses.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"response2\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5757,\n        \"samples\": [\n          \"The policyholder provided additional information regarding the wildfire incident, indicating that they have discovered further smoke damage within the home that was not initially reported. They also noted that some personal belongings, particularly textiles and electronics, have been adversely affected by the smoke intrusion. The policyholder expressed concerns about the potential for lingering odors and health implications. A detailed inventory of damaged items is being compiled, and further assessment of the structural integrity will be required to determine any additional impacts on the claim.\",\n          \"The policyholder has completed their assessment of the wildfire-related damage and reported that smoke intrusion has caused significant issues inside the home, particularly in the kitchen where several electronics were rendered non-functional due to soot and ash contamination. After discussing the findings, the policyholder has decided to handle the cleanup and replacement of affected items independently, therefore, this claim will be closed as they have chosen not to pursue further insurance assistance at this time.\",\n          \"The policyholder reported no significant updates since the initial call, maintaining concerns about the ongoing displacement and the timeline for repairs. They continue to await contractor assessments for both the main structure and outdoor damages, including the warped lighting fixtures, and expressed frustration over the lack of information regarding the overall recovery process. Further updates will be documented as they arise.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sus\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"high\",\n          \"very low\",\n          \"average\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cert\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"average\",\n          \"very high\",\n          \"high\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fraud_prob\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1793537889124201,\n        \"min\": 0.057,\n        \"max\": 0.987,\n        \"num_unique_values\": 792,\n        \"samples\": [\n          0.941,\n          0.338,\n          0.366\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"repudiation\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          true,\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['response'].value_counts()"
      ],
      "metadata": {
        "id": "4e5RP9eZO06K",
        "outputId": "85b500b4-ce58-4d7b-8a72-2f90e92b8f84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 889
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "response\n",
              "The policyholder reported damage due to a nearby wildfire, affecting both the main structure and personal property. Notably, there is moderate staining observed on the ceiling and upper walls from smoke exposure, which will require assessment and potential remediation. Additionally, the insured mentioned incurring storage unit fees for salvaged personal belongings, which could be relevant for further investigation into recovery and reimbursement options. Claim development will focus on these aspects to ensure a comprehensive evaluation of damages and associated costs.                                                                                                                                                                                   1\n",
              "The claimant reported damage to the exterior of their home following an incident involving burning pine cones that popped and scattered embers, igniting nearby vegetation. The policyholder has a coverage limit of $261,000 for the main structure and $121,000 for contents, which may be relevant if further damages are identified. Additional details regarding the extent of the fire and any potential impact on nearby structures remain unclear at this stage and will require further investigation.                                                                                                                                                                                                                                                                  1\n",
              "The insured reported an incident involving an overheating issue caused by a malfunctioning dryer door switch, which resulted in damage to the laundry room floor and surrounding areas. The insured confirmed they have coverage for both the main structure and contents, and they are currently assessing the extent of the damage. Additional information on repairs and any personal property affected will be collected as the claim progresses.                                                                                                                                                                                                                                                                                                                            1\n",
              "The policyholder reported an external fire incident involving a brush pile in the backyard that spontaneously combusted, resulting in damage to the surrounding area. They have coverage for both the main structure and contents, and while the full extent of the damages is still being assessed, initial observations indicate potential impacts on nearby landscaping and structures. Further investigation will be required to determine the full scope of the claim.                                                                                                                                                                                                                                                                                                      1\n",
              "Claimant reported an internal fire incident in their garage, caused by an E-bike battery igniting, resulting in smoke damage throughout the main structure. The homeowner's total coverage for the dwelling is $383,000, with contents coverage at $184,000, indicating potential extensive damage to both property and personal belongings. Further investigation will be necessary to assess the full extent of the damage and determine the appropriate response for the claim. Claimant will provide additional details regarding any affected personal items once the fire department completes their report.                                                                                                                                                               1\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ..\n",
              "The claimant reported an internal fire incident in their home in Huntington Park, caused by an e-cig battery explosion on the coffee table, resulting in damage to surrounding furniture and potential smoke damage throughout the living area. The policyholder mentioned that while the fire was quickly contained, they are concerned about the extent of the damage and whether their contents coverage will adequately address the losses. Further investigation will be necessary to assess the full impact on both the structure and personal property, given the total policy limits of $311,000 for the main structure and $203,000 for contents. The claimant will provide additional details as they gather information on damaged items and any related expenses.    1\n",
              "The claimant reported an external fire incident involving their property, where it appears that kids were playing with firecrackers near the carport, which may have contributed to the blaze. The policyholder has coverage for the main structure and contents, and they noted that the fire caused damage to both areas. Further investigation is needed to determine the extent of the damage and any potential liability issues related to the activities of the children. The claimant mentioned that they had recently used their outdoor fireplace, but did not specify whether the chimney was clogged with ash at the time of the incident. Additional details will be gathered in follow-up communications.                                                           1\n",
              "The policyholder reported an internal fire incident involving a worn-out power strip that was located near a water source, which sparked and subsequently caused damage to the main structure. While the total limit for the main structure is $397,000 and contents coverage stands at $160,000, the extent of the damage remains unclear at this initial stage. The caller mentioned that the fire affected both the electrical system and surrounding areas, and further assessment will be necessary to determine the full scope of the damages and any potential loss of personal property.                                                                                                                                                                                 1\n",
              "The policyholder reported an external fire incident in Huntington Park, which appears to have been caused by illegal yard debris burning that got out of control. The main structure is insured for a total limit of $266,000, with contents coverage of $115,000. Further investigation will be necessary to assess the extent of the damage and verify the circumstances surrounding the fire. The policy specifics were confirmed with the insured, and additional details will be gathered as the claim progresses.                                                                                                                                                                                                                                                          1\n",
              "The claimant reported an incident involving a fire that reportedly originated from a storage shed containing various paint cans, which ignited under unclear circumstances. There is also mention of an ATV engine catching fire near a separate tool shed, as well as discarded aerosol cans that allegedly exploded in a dumpster nearby. The details shared raise some questions regarding the sequence of events and the presence of multiple ignition sources, suggesting that further investigation may be warranted to clarify the situation surrounding the fire. The policyholder has coverage for both the main structure and contents, and the unique aspects of this claim will be taken into account as we proceed.                                                 1\n",
              "Name: count, Length: 5757, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>response</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>The policyholder reported damage due to a nearby wildfire, affecting both the main structure and personal property. Notably, there is moderate staining observed on the ceiling and upper walls from smoke exposure, which will require assessment and potential remediation. Additionally, the insured mentioned incurring storage unit fees for salvaged personal belongings, which could be relevant for further investigation into recovery and reimbursement options. Claim development will focus on these aspects to ensure a comprehensive evaluation of damages and associated costs.</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>The claimant reported damage to the exterior of their home following an incident involving burning pine cones that popped and scattered embers, igniting nearby vegetation. The policyholder has a coverage limit of $261,000 for the main structure and $121,000 for contents, which may be relevant if further damages are identified. Additional details regarding the extent of the fire and any potential impact on nearby structures remain unclear at this stage and will require further investigation.</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>The insured reported an incident involving an overheating issue caused by a malfunctioning dryer door switch, which resulted in damage to the laundry room floor and surrounding areas. The insured confirmed they have coverage for both the main structure and contents, and they are currently assessing the extent of the damage. Additional information on repairs and any personal property affected will be collected as the claim progresses.</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>The policyholder reported an external fire incident involving a brush pile in the backyard that spontaneously combusted, resulting in damage to the surrounding area. They have coverage for both the main structure and contents, and while the full extent of the damages is still being assessed, initial observations indicate potential impacts on nearby landscaping and structures. Further investigation will be required to determine the full scope of the claim.</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Claimant reported an internal fire incident in their garage, caused by an E-bike battery igniting, resulting in smoke damage throughout the main structure. The homeowner's total coverage for the dwelling is $383,000, with contents coverage at $184,000, indicating potential extensive damage to both property and personal belongings. Further investigation will be necessary to assess the full extent of the damage and determine the appropriate response for the claim. Claimant will provide additional details regarding any affected personal items once the fire department completes their report.</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>The claimant reported an internal fire incident in their home in Huntington Park, caused by an e-cig battery explosion on the coffee table, resulting in damage to surrounding furniture and potential smoke damage throughout the living area. The policyholder mentioned that while the fire was quickly contained, they are concerned about the extent of the damage and whether their contents coverage will adequately address the losses. Further investigation will be necessary to assess the full impact on both the structure and personal property, given the total policy limits of $311,000 for the main structure and $203,000 for contents. The claimant will provide additional details as they gather information on damaged items and any related expenses.</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>The claimant reported an external fire incident involving their property, where it appears that kids were playing with firecrackers near the carport, which may have contributed to the blaze. The policyholder has coverage for the main structure and contents, and they noted that the fire caused damage to both areas. Further investigation is needed to determine the extent of the damage and any potential liability issues related to the activities of the children. The claimant mentioned that they had recently used their outdoor fireplace, but did not specify whether the chimney was clogged with ash at the time of the incident. Additional details will be gathered in follow-up communications.</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>The policyholder reported an internal fire incident involving a worn-out power strip that was located near a water source, which sparked and subsequently caused damage to the main structure. While the total limit for the main structure is $397,000 and contents coverage stands at $160,000, the extent of the damage remains unclear at this initial stage. The caller mentioned that the fire affected both the electrical system and surrounding areas, and further assessment will be necessary to determine the full scope of the damages and any potential loss of personal property.</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>The policyholder reported an external fire incident in Huntington Park, which appears to have been caused by illegal yard debris burning that got out of control. The main structure is insured for a total limit of $266,000, with contents coverage of $115,000. Further investigation will be necessary to assess the extent of the damage and verify the circumstances surrounding the fire. The policy specifics were confirmed with the insured, and additional details will be gathered as the claim progresses.</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>The claimant reported an incident involving a fire that reportedly originated from a storage shed containing various paint cans, which ignited under unclear circumstances. There is also mention of an ATV engine catching fire near a separate tool shed, as well as discarded aerosol cans that allegedly exploded in a dumpster nearby. The details shared raise some questions regarding the sequence of events and the presence of multiple ignition sources, suggesting that further investigation may be warranted to clarify the situation surrounding the fire. The policyholder has coverage for both the main structure and contents, and the unique aspects of this claim will be taken into account as we proceed.</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5757 rows  1 columns</p>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Based on Info159 - Natural Language Processing HW 1 by Prof. David Bamman\n",
        "class FeaturizedDataLoader:\n",
        "    def __init__(self,\n",
        "                 csv_file,\n",
        "                 feature_method,\n",
        "                 test_size=0.2,\n",
        "                 random_state=42,\n",
        "                 min_feature_count=1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_file: Path to your CSV file\n",
        "            feature_method: A function that takes in text and returns a dictionary of features\n",
        "            test_size: Fraction of data to hold out for test\n",
        "            random_state: Random seed for reproducible splits\n",
        "            min_feature_count: Minimum frequency of a feature for it to be included in the vocabulary\n",
        "        \"\"\"\n",
        "        self.feature_vocab = {}\n",
        "        self.feature_method = feature_method\n",
        "        self.min_feature_count = min_feature_count\n",
        "\n",
        "        # 1) Load the data from the CSV\n",
        "        data = self.load_data(csv_file)\n",
        "\n",
        "        # 2) Split into train and test\n",
        "        train_data, test_data = train_test_split(data,\n",
        "                                                 test_size=test_size,\n",
        "                                                 random_state=random_state)\n",
        "\n",
        "        # 3) Process the data\n",
        "        self.trainX, self.trainY, self.trainOrig = self.process(train_data, training=True)\n",
        "        self.testX, self.testY, self.testOrig   = self.process(test_data, training=False)\n",
        "\n",
        "    def load_data(self, csv_file):\n",
        "        \"\"\"\n",
        "        Loads the CSV and produces a list of tuples: (idd, label, text),\n",
        "        where label = 'repudiation' and text = concatenation of 'response' and 'response2'.\n",
        "        \"\"\"\n",
        "        df = pd.read_csv(csv_file)\n",
        "        data = []\n",
        "\n",
        "        # You can decide how you want to define `idd`; here, we'll just use the row index.\n",
        "        for idx, row in df.iterrows():\n",
        "            idd = idx\n",
        "            label = row['repudiation']\n",
        "\n",
        "            # Concatenate response + response2 (handle NaN or missing values as needed)\n",
        "            # Use str() to ensure concatenation works even if they are numeric or missing.\n",
        "            response_text = str(row['response']) if pd.notnull(row['response']) else \"\"\n",
        "            response2_text = str(row['response2']) if pd.notnull(row['response2']) else \"\"\n",
        "            text = response_text + \" \" + response2_text\n",
        "\n",
        "            data.append((idd, label, text))\n",
        "        return data\n",
        "\n",
        "    def featurize(self, data):\n",
        "        \"\"\"\n",
        "        Featurizes the data using the user-defined feature_method.\n",
        "        \"\"\"\n",
        "        featurized_data = []\n",
        "        for idd, label, text in data:\n",
        "            feats = self.feature_method(text)\n",
        "            featurized_data.append((label, feats))\n",
        "        return featurized_data\n",
        "\n",
        "    def process(self, data, training=False):\n",
        "        \"\"\"\n",
        "        Converts data into a sparse matrix (dok_matrix) X and label array Y.\n",
        "        If training=True, also builds/updates the feature vocabulary.\n",
        "        \"\"\"\n",
        "        # data is a list of (idd, label, text)\n",
        "        # We only need: label, text to featurize.\n",
        "        # Keep original_data for reference or debugging\n",
        "        original_data = data\n",
        "\n",
        "        # 1) Featurize\n",
        "        featurized_data = self.featurize(data)  # -> list of (label, {feat: count, ...})\n",
        "\n",
        "        # 2) Build vocab if training\n",
        "        if training:\n",
        "            fid = 0\n",
        "            feature_doc_count = Counter()\n",
        "            for label, feats in featurized_data:\n",
        "                for feat in feats:\n",
        "                    feature_doc_count[feat] += 1\n",
        "\n",
        "            for feat, count in feature_doc_count.items():\n",
        "                if count >= self.min_feature_count:\n",
        "                    self.feature_vocab[feat] = fid\n",
        "                    fid += 1\n",
        "\n",
        "        # 3) Create the sparse matrix X, label array Y\n",
        "        F = len(self.feature_vocab)\n",
        "        D = len(featurized_data)\n",
        "\n",
        "        X = sparse.dok_matrix((D, F), dtype=float)\n",
        "        Y = [None]*D\n",
        "\n",
        "        for idx, (label, feats) in enumerate(featurized_data):\n",
        "            for feat, val in feats.items():\n",
        "                if feat in self.feature_vocab:\n",
        "                    X[idx, self.feature_vocab[feat]] = val\n",
        "            Y[idx] = label\n",
        "\n",
        "        return X, Y, original_data\n",
        "\n",
        "    def load_test(self, dataFile):\n",
        "        \"\"\"\n",
        "        If you need a separate method to load an external test CSV in the future,\n",
        "        similar to your old code, you could adapt it here.\n",
        "        This is just a placeholder to show how it might be adapted.\n",
        "        \"\"\"\n",
        "        # You can re-use the same logic:\n",
        "        # 1) load new data (with the same columns)\n",
        "        # 2) featurize\n",
        "        # 3) create X\n",
        "        # 4) but you won't update self.feature_vocab\n",
        "        pass\n"
      ],
      "metadata": {
        "id": "M0d6GCJ6OjDu"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a Simple Classifier"
      ],
      "metadata": {
        "id": "jah0wUpAOjd8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoding the Input"
      ],
      "metadata": {
        "id": "9tnAeVVIOsQW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bag of Words"
      ],
      "metadata": {
        "id": "MwGeEplUOv6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bag_of_words(text):\n",
        "    # Here the `feats` dict should contain the features -- the key should be the feature name,\n",
        "    # and the value is the feature value.  See `simple_featurize` for an example.\n",
        "\n",
        "    feats = {}\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    for word in words:\n",
        "      word=word.lower()\n",
        "\n",
        "      feats[word] = 1\n",
        "    return feats\n"
      ],
      "metadata": {
        "id": "6L6UuSHuOr1N"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BoW_Loader = FeaturizedDataLoader('/content/2025-CAS-RPM-NN-GenAI-Workshop/prompt.csv', bag_of_words)"
      ],
      "metadata": {
        "id": "EWf99Dxn_j5H"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's take a look\n",
        "\n",
        "# dimensions\n",
        "D, F = BoW_Loader.trainX.shape\n",
        "print(\"number of rows in train X:\", D)\n",
        "print(\"number of features:\", F)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VF8PVy-k5FrV",
        "outputId": "62421f3c-4a67-4324-97d3-87524e252294"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of rows in train X: 4605\n",
            "number of features: 4799\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embeddings"
      ],
      "metadata": {
        "id": "kf--DV7qOzF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch embeddings\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove*.zip"
      ],
      "metadata": {
        "id": "aqMd0rDaO1WH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa054e91-59f5-484b-ba1d-efc7e28c1d3e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-01 06:46:22--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2025-03-01 06:46:23--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2025-03-01 06:46:23--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: glove.6B.zip\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  3.92MB/s    in 5m 25s  \n",
            "\n",
            "2025-03-01 06:51:48 (2.53 MB/s) - glove.6B.zip saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_index = {}\n",
        "f = open('glove.6B.100d.txt')\n",
        "for line in f:\n",
        "    values = line.split(' ')\n",
        "    word = values[0] ## The first entry is the word\n",
        "    coefs = np.asarray(values[1:], dtype='float32') ## These are the vecotrs representing the embedding for the word\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZO-13ZoM7Jp",
        "outputId": "1b198da8-0da9-445d-8afa-2e3fd101a78e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To make use of these embeddings, we need to ensure we have a mapping of words to indices\n",
        "# Let's define a simple word based tokenizer to work with these embeddings\n",
        "\n",
        "class WordTokenizer:\n",
        "    def __init__(self, documents=None, vocab_size=100000):\n",
        "        if documents:\n",
        "          self.id2token = Dictionary(\n",
        "              [nltk.word_tokenize(document.lower()) for document in documents],\n",
        "              prune_at=vocab_size\n",
        "          )\n",
        "          self.id2token.filter_extremes(no_below=1, no_above=1, keep_n=vocab_size)\n",
        "        else:\n",
        "          self.id2token = Dictionary(prune_at = vocab_size)\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    def __call__(self, text):\n",
        "        return self.tokenize(text)\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        tokens = nltk.word_tokenize(text.lower())\n",
        "        return self.id2token.doc2idx(tokens, unknown_word_index = self.get_vocab_size())\n",
        "\n",
        "    def get_vocab(self):\n",
        "        return self.id2token.token2id\n",
        "\n",
        "    def get_vocab_size(self):\n",
        "        return len(self.get_vocab())\n",
        "\n",
        "    def get_word_id(self, word):\n",
        "        return self.get_vocab()[word]\n",
        "\n",
        "    def get_word(self, idx):\n",
        "        return self.id2token.get(idx, None)\n",
        "\n",
        "    def add_documents(self, documents):\n",
        "        self.id2token.add_documents([nltk.word_tokenize(document.lower()) for document in documents], prune_at=self.vocab_size)\n",
        "        self.id2token.filter_extremes(no_below=1, no_above=1, keep_n=self.vocab_size)"
      ],
      "metadata": {
        "id": "VdJe-WrFOpRS"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's build an embedding matrix based on this tokenizer and the embeddings we loaded earlier\n",
        "\n",
        "def create_embedding_matrix(tokenizer: WordTokenizer, embeddings_index, embedding_dim=100):\n",
        "    vocab_size = tokenizer.get_vocab_size() + 1  # +1 for OOV token if needed\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim)).astype('float32')\n",
        "\n",
        "    for word in tokenizer.get_vocab():\n",
        "        if word in embeddings_index:\n",
        "            idx = tokenizer.get_word_id(word)\n",
        "            embedding_matrix[idx] = embeddings_index[word]\n",
        "\n",
        "    return embedding_matrix\n",
        "\n"
      ],
      "metadata": {
        "id": "ElGX-mqzPNtF"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# recall our dataloader stores the original training data in the trainOrig attribute as a list of (idd, label, text)\n",
        "\n",
        "# now we extract the text from this\n",
        "\n",
        "trainOrigDocuments = [x[2] for x in BoW_Loader.trainOrig]\n",
        "\n",
        "word_tokenizer = WordTokenizer(trainOrigDocuments, 10000)\n",
        "embedding_matrix = create_embedding_matrix(word_tokenizer, embeddings_index)"
      ],
      "metadata": {
        "id": "4KJ-L_vSrIpv"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzVoP1yZAbGx",
        "outputId": "7fc83cc1-2fa2-4c4d-8946-c235e477c9e4"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4800, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(embedding_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7Eeg1wl5dAS",
        "outputId": "e893c0be-83a2-4fc7-ca55-475018522977"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.10767 ,  0.11053 ,  0.59812 , ..., -0.83155 ,  0.45293 ,\n",
              "         0.082577],\n",
              "       [-0.33979 ,  0.20941 ,  0.46348 , ..., -0.23394 ,  0.47298 ,\n",
              "        -0.028803],\n",
              "       [-0.27086 ,  0.044006, -0.02026 , ..., -0.4923  ,  0.63687 ,\n",
              "         0.23642 ],\n",
              "       ...,\n",
              "       [-0.26265 , -0.45791 ,  0.31674 , ...,  0.018427,  0.025385,\n",
              "         0.45626 ],\n",
              "       [-0.14031 , -0.45892 , -0.14656 , ..., -0.04242 ,  0.73645 ,\n",
              "         0.54508 ],\n",
              "       [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
              "         0.      ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenizer.get_word(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0obaR54W5e3q",
        "outputId": "ab6582b8-8104-4315-d831-4efdfde7b7fc"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "','"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_index['!']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01FK6moS57vi",
        "outputId": "f395c669-9654-4e4d-f67a-6e4f195181c1"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.38472  ,  0.49351  ,  0.49096  , -1.5434   , -0.33614  ,\n",
              "        0.6222   ,  0.32265  ,  0.075331 ,  0.65591  , -0.23517  ,\n",
              "        1.2114   ,  0.06193  , -0.62004  ,  0.31371  ,  0.38948  ,\n",
              "       -0.24381  , -0.065643 ,  0.58797  , -0.86382  ,  0.63166  ,\n",
              "        0.68363  ,  0.39647  , -0.62388  , -0.25094  ,  0.92831  ,\n",
              "        1.5152   , -0.43917  ,  0.22249  ,  1.3695   , -0.53098  ,\n",
              "        0.39811  ,  0.77114  ,  0.49043  ,  0.58853  ,  0.2376   ,\n",
              "        0.3162   , -0.011962 , -0.047074 ,  0.34585  , -1.2944   ,\n",
              "        0.18597  ,  0.27002  , -0.70602  , -0.20652  , -0.25194  ,\n",
              "       -0.4868   , -0.71538  , -0.23887  , -0.041612 , -0.55488  ,\n",
              "       -0.54226  ,  0.21236  ,  0.025341 ,  0.96517  , -0.88183  ,\n",
              "       -1.8681   ,  0.32657  ,  1.1689   ,  1.1759   , -0.17393  ,\n",
              "       -0.3371   ,  0.87535  , -1.0114   , -0.6181   ,  1.008    ,\n",
              "        0.31506  ,  0.24417  ,  0.064393 ,  0.33678  ,  0.33632  ,\n",
              "        0.45975  ,  0.22813  , -0.37505  , -0.37508  ,  0.089301 ,\n",
              "        0.53862  ,  0.039714 , -0.0036392, -0.25023  , -0.18224  ,\n",
              "        0.42731  , -0.79118  , -0.29409  , -0.40693  , -1.0908   ,\n",
              "       -0.16476  , -0.41458  , -0.67899  ,  0.28319  ,  0.30937  ,\n",
              "        0.49304  , -0.067002 ,  0.50222  ,  0.73959  , -0.4735   ,\n",
              "       -0.47342  , -0.20242  ,  0.026263 ,  0.39052  ,  0.52217  ],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining a Model"
      ],
      "metadata": {
        "id": "glJl1wRnO5o2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Regression"
      ],
      "metadata": {
        "id": "2fFXI6SyO8c2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "L2_regularization_strength = 1.0"
      ],
      "metadata": {
        "id": "A6pILdP1xscL"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a Logistic Regression https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
        "log_reg = LogisticRegression(C = L2_regularization_strength, max_iter=1000)\n",
        "log_reg.fit(BoW_Loader.trainX, BoW_Loader.trainY)\n",
        "\n",
        "# score it against our data\n",
        "training_accuracy = log_reg.score(BoW_Loader.trainX, BoW_Loader.trainY)\n",
        "validation_accuracy = log_reg.score(BoW_Loader.testX, BoW_Loader.testY)\n",
        "\n",
        "print(\"Method: %s, Features: %s, Train accuracy: %.3f, validation accuracy: %.3f\" % (BoW_Loader.feature_method.__name__, F, training_accuracy, validation_accuracy))"
      ],
      "metadata": {
        "id": "NjFeTVyAO7t4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1879a409-cd05-423a-a516-7579413276ed"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Method: bag_of_words, Features: 4799, Train accuracy: 0.997, Dev accuracy: 0.974\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's get a grasp of the distribution to frame these results\n",
        "\n",
        "print(\"Train Positive Proportion:\", sum(BoW_Loader.trainY)/len(BoW_Loader.trainY))\n",
        "print(\"Train Negative Proportion:\", 1 - sum(BoW_Loader.trainY)/len(BoW_Loader.trainY))\n",
        "print(\"Train Positive Count:\", sum(BoW_Loader.trainY))\n",
        "print(\"Train Count:\", len(BoW_Loader.trainY))\n",
        "print(\"Test Positive Proportion:\", sum(BoW_Loader.testY)/len(BoW_Loader.testY))\n",
        "print(\"Test Negative Proportion:\", 1 - sum(BoW_Loader.testY)/len(BoW_Loader.testY))\n",
        "print(\"Test Positive Count:\", sum(BoW_Loader.testY))\n",
        "print(\"Test Count:\", len(BoW_Loader.testY))"
      ],
      "metadata": {
        "id": "Tge7NEbke4d4",
        "outputId": "ed28c311-20b0-4e28-88e7-599da95e795e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Positive Proportion: 0.02975027144408252\n",
            "Train Negative Proportion: 0.9702497285559175\n",
            "Train Positive Count: 137\n",
            "Train Count: 4605\n",
            "Test Positive Proportion: 0.019097222222222224\n",
            "Test Negative Proportion: 0.9809027777777778\n",
            "Test Positive Count: 22\n",
            "Test Count: 1152\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multilayer Perceptron"
      ],
      "metadata": {
        "id": "_KWLfioXPAgu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Torch Dataset and DataLoader from our data\n",
        "\n",
        "class BoW_Dataset(Dataset):\n",
        "    def __init__(self, X, Y):\n",
        "        self.X = X.toarray().astype(np.float32)\n",
        "        self.Y = Y\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.X.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.X[idx]\n",
        "        y = self.Y[idx]\n",
        "\n",
        "        # specific to our data\n",
        "        if y == True:\n",
        "          y = 1\n",
        "        else:\n",
        "          y = 0\n",
        "        return x, np.array([y]).astype(np.float32)\n",
        "\n",
        "train_dataset = BoW_Dataset(BoW_Loader.trainX, BoW_Loader.trainY)\n",
        "valid_dataset = BoW_Dataset(BoW_Loader.testX, BoW_Loader.testY)\n",
        "\n",
        "# dataloader = DataLoader(train_dataset, batch_size=2)"
      ],
      "metadata": {
        "id": "VLZBxQGd0iQI"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Linear(F, 1),\n",
        "    # nn.Sigmoid()\n",
        ")\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4wNoyh4zYAT",
        "outputId": "ce449dd5-50e6-45d5-f75e-d031b01fa9de"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=4799, out_features=1, bias=True)\n",
            "  (1): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO figure out default hyperparams\n",
        "\n",
        "def run_training_loop(model, batch_size=32, n_epochs=10, lr=1e-3, weight_decay = 1e-4):\n",
        "\n",
        "    # We could write our training procedure manually and directly index the `Dataset` objects,\n",
        "    # but the `DataLoader` object conveniently creates an iterable for automatically creating random minibatches:\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Choose Adam as the optimizer\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay = weight_decay)\n",
        "\n",
        "    # Use the cross entropy loss function\n",
        "    # loss_fn = nn.CrossEntropyLoss()\n",
        "    loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([97/3]))\n",
        "    # store metrics\n",
        "    train_loss_history = np.zeros([n_epochs, 1])\n",
        "    valid_accuracy_history = np.zeros([n_epochs, 1])\n",
        "    valid_loss_history = np.zeros([n_epochs, 1])\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "\n",
        "        # Some layers, such as Dropout, behave differently during training\n",
        "        model.train()\n",
        "\n",
        "        train_loss = 0\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            # Erase accumulated gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            output = model(data)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = loss_fn(output, target)\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Weight update\n",
        "            optimizer.step()\n",
        "\n",
        "        train_loss_history[epoch] = train_loss / len(train_loader.dataset)\n",
        "\n",
        "        # Track loss each epoch\n",
        "        print('Train Epoch: %d  Average loss: %.4f' %\n",
        "              (epoch + 1,  train_loss_history[epoch]))\n",
        "\n",
        "        # Putting layers like Dropout into evaluation mode\n",
        "        model.eval()\n",
        "\n",
        "        valid_loss = 0\n",
        "        correct = 0\n",
        "\n",
        "        # Turning off automatic differentiation\n",
        "        with torch.no_grad():\n",
        "            for data, target in valid_loader:\n",
        "                output = model(data)\n",
        "                valid_loss += loss_fn(output, target).item()  # Sum up batch loss\n",
        "                pred = torch.round(output)\n",
        "                # pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max class score\n",
        "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "        valid_loss_history[epoch] = valid_loss / len(valid_loader.dataset)\n",
        "        valid_accuracy_history[epoch] = correct / len(valid_loader.dataset)\n",
        "\n",
        "        print('Valid set: Average loss: %.4f, Accuracy: %d/%d (%.4f)\\n' %\n",
        "              (valid_loss_history[epoch], correct, len(valid_loader.dataset),\n",
        "              100. * valid_accuracy_history[epoch]))\n",
        "\n",
        "    return model, train_loss_history, valid_loss_history, valid_accuracy_history"
      ],
      "metadata": {
        "id": "W4QBL6p80LQ2"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_nn, train_loss_history, valid_loss_history, valid_accuracy_history = run_training_loop(model, batch_size=32, n_epochs=10, lr=1e-2, weight_decay = 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fR8ZB9n7lL_",
        "outputId": "f7f3ef38-e924-41f4-f2d6-2e15304b8928"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-77-0e33d6c9b6be>:45: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print('Train Epoch: %d  Average loss: %.4f' %\n",
            "<ipython-input-77-0e33d6c9b6be>:66: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print('Valid set: Average loss: %.4f, Accuracy: %d/%d (%.4f)\\n' %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1  Average loss: 0.0307\n",
            "Valid set: Average loss: 0.0356, Accuracy: 1076/1152 (93.4028)\n",
            "\n",
            "Train Epoch: 2  Average loss: 0.0306\n",
            "Valid set: Average loss: 0.0351, Accuracy: 1099/1152 (95.3993)\n",
            "\n",
            "Train Epoch: 3  Average loss: 0.0305\n",
            "Valid set: Average loss: 0.0351, Accuracy: 1101/1152 (95.5729)\n",
            "\n",
            "Train Epoch: 4  Average loss: 0.0305\n",
            "Valid set: Average loss: 0.0350, Accuracy: 1111/1152 (96.4410)\n",
            "\n",
            "Train Epoch: 5  Average loss: 0.0305\n",
            "Valid set: Average loss: 0.0351, Accuracy: 1101/1152 (95.5729)\n",
            "\n",
            "Train Epoch: 6  Average loss: 0.0305\n",
            "Valid set: Average loss: 0.0351, Accuracy: 1098/1152 (95.3125)\n",
            "\n",
            "Train Epoch: 7  Average loss: 0.0305\n",
            "Valid set: Average loss: 0.0351, Accuracy: 1097/1152 (95.2257)\n",
            "\n",
            "Train Epoch: 8  Average loss: 0.0305\n",
            "Valid set: Average loss: 0.0350, Accuracy: 1106/1152 (96.0069)\n",
            "\n",
            "Train Epoch: 9  Average loss: 0.0305\n",
            "Valid set: Average loss: 0.0350, Accuracy: 1106/1152 (96.0069)\n",
            "\n",
            "Train Epoch: 10  Average loss: 0.0305\n",
            "Valid set: Average loss: 0.0350, Accuracy: 1109/1152 (96.2674)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Linear(F, 1024),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(1024, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256, 1),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSxczPXbBeyy",
        "outputId": "c12c60b6-f711-4be0-b5aa-dfe12aa5eb95"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=4799, out_features=1024, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "  (3): ReLU()\n",
            "  (4): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (5): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(word_tokenizer.get_vocab())\n",
        "word_tokenizer.vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OX-DC9wb_Vv9",
        "outputId": "d0bc6934-3cfa-4feb-c7c1-958829fd8421"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mlp, train_loss_history, valid_loss_history, valid_accuracy_history = run_training_loop(model, batch_size=32, n_epochs=5, lr=1e-3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJxE1RDhAcXl",
        "outputId": "6efb2270-f2b4-4d59-a4a3-1b860c823150"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-49-031e17bf88a2>:45: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print('Train Epoch: %d  Average loss: %.4f' %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1  Average loss: 0.0004\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-49-031e17bf88a2>:66: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print('Valid set: Average loss: %.4f, Accuracy: %d/%d (%.4f)\\n' %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valid set: Average loss: 0.0000, Accuracy: 1152/1152 (100.0000)\n",
            "\n",
            "Train Epoch: 2  Average loss: 0.0000\n",
            "Valid set: Average loss: 0.0000, Accuracy: 1152/1152 (100.0000)\n",
            "\n",
            "Train Epoch: 3  Average loss: 0.0000\n",
            "Valid set: Average loss: 0.0000, Accuracy: 1152/1152 (100.0000)\n",
            "\n",
            "Train Epoch: 4  Average loss: 0.0000\n",
            "Valid set: Average loss: 0.0000, Accuracy: 1152/1152 (100.0000)\n",
            "\n",
            "Train Epoch: 5  Average loss: 0.0000\n",
            "Valid set: Average loss: 0.0000, Accuracy: 1152/1152 (100.0000)\n",
            "\n",
            "Train Epoch: 6  Average loss: 0.0000\n",
            "Valid set: Average loss: 0.0000, Accuracy: 1152/1152 (100.0000)\n",
            "\n",
            "Train Epoch: 7  Average loss: 0.0000\n",
            "Valid set: Average loss: 0.0000, Accuracy: 1152/1152 (100.0000)\n",
            "\n",
            "Train Epoch: 8  Average loss: 0.0000\n",
            "Valid set: Average loss: 0.0000, Accuracy: 1152/1152 (100.0000)\n",
            "\n",
            "Train Epoch: 9  Average loss: 0.0000\n",
            "Valid set: Average loss: 0.0000, Accuracy: 1152/1152 (100.0000)\n",
            "\n",
            "Train Epoch: 10  Average loss: 0.0000\n",
            "Valid set: Average loss: 0.0000, Accuracy: 1152/1152 (100.0000)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convolutional Neural Network"
      ],
      "metadata": {
        "id": "1adjh_i1PFPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's define a dataset using our word tokenizer\n",
        "\n",
        "\n",
        "class Tokenized_Dataset(Dataset):\n",
        "    def __init__(self, X, Y, tokenizer, max_seq_len=500):\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.X[idx]\n",
        "        y = self.Y[idx]\n",
        "\n",
        "        # use the tokenizer\n",
        "\n",
        "        x = self.tokenizer(x)\n",
        "\n",
        "        # Pad or truncate to max_seq_len\n",
        "        if len(x) < self.max_seq_len:\n",
        "            x = x + [self.tokenizer.get_vocab_size()] * (self.max_seq_len - len(x))\n",
        "        else:\n",
        "            x = x[:self.max_seq_len]\n",
        "        x = np.array(x)\n",
        "        # specific to our data\n",
        "        if y == True:\n",
        "          y = 1\n",
        "        else:\n",
        "          y = 0\n",
        "        return x, np.array([y]).astype(np.float32)\n",
        "\n",
        "trainOrigDocuments = [x[2] for x in BoW_Loader.trainOrig]\n",
        "validOrigDocuments = [x[2] for x in BoW_Loader.testOrig]\n",
        "\n",
        "train_dataset = Tokenized_Dataset(trainOrigDocuments, BoW_Loader.trainY, word_tokenizer, max_seq_len = 500)\n",
        "valid_dataset = Tokenized_Dataset(validOrigDocuments, BoW_Loader.testY, word_tokenizer, max_seq_len = 500)"
      ],
      "metadata": {
        "id": "T-WoDFe1EfSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's try to use the embeddings we loaded earlier now\n",
        "model = nn.Sequential(\n",
        "    nn.Embedding.from_pretrained(torch.from_numpy(embedding_matrix)) # by default this is frozen ie will not be updated during training\n",
        ")\n",
        "print(model)"
      ],
      "metadata": {
        "id": "1RD3FY82PHLI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47ae4a0a-0349-41ab-80c5-90933b788618"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Embedding(10001, 100)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's mimic what will happen with our embeddings\n",
        "max_seq_len = 500\n",
        "\n",
        "x = BoW_Loader.trainOrig[0][2]\n",
        "x = word_tokenizer(x)\n",
        "padding_token = word_tokenizer.get_vocab_size()\n",
        "\n",
        "# Pad or truncate to max_seq_len\n",
        "if len(x) < max_seq_len:\n",
        "    x = x + [word_tokenizer.get_vocab_size()] * (max_seq_len - len(x))\n",
        "else:\n",
        "    x = x[:max_seq_len]\n",
        "print(\"padding and unknown token is:\", padding_token)\n",
        "print(\"Tokenized Sequence is:\")\n",
        "print(np.array(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Kf1HoMS9d5W",
        "outputId": "1b593812-c33f-47e9-c756-30f9da5649fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "padding and unknown token is: 10000\n",
            "Tokenized Sequence is:\n",
            "[   50    94     4    75     1   126   156   130    89   105     8     6\n",
            "     3   133    19    63     4    52   153    68   130   127    78    16\n",
            "   110   129    19   113   129   155    55    90   152    88    26    17\n",
            "     9     7     3   130    84    69    92    51    13   101     4   122\n",
            "    66   148   155    55   118    16   126   159    42   130    85   140\n",
            "    11    69    67    25   130    84    69   147   111     5    68    15\n",
            "    96   141   131     2    77   130    98    64   149   139     3     4\n",
            "   160    30    61   130   116    57   137   130   108    21   138     5\n",
            "    64    80   129     5   130    84    40    92    71   160    27   156\n",
            "    86     4    91    45     8    70    76   160   117   144   150    69\n",
            "    58    69    10    74    93   119   129    54    15    22   129    69\n",
            "    49    15   145    46     5   130   120    72    93    22    53    29\n",
            "   125    73    68    47   154   121     5    92    51    13   102   158\n",
            "    62   115    70   139    48    93    33     4    20   130    34   157\n",
            "    69    14     5   135   129    23   132    18    93    10    84   123\n",
            "    97   130   127   155    24   145    38     5    70     1   130    72\n",
            "    93   127   129   160    60   139    81   143   130    79    68   161\n",
            "    82    12     5     2    87   146    93    70    69   106    37    35\n",
            "     4    28   107   112    95     0     3    65   160    39    56    31\n",
            "   130    84     4   133    69    98   147    35   100     8   151   130\n",
            "   140    36    19   128   139    43    99    97   132    32   104     5\n",
            "    15   142   124    44   129   114   109   160    68   130    83     5\n",
            "    59    94   139   130   103   129   136    93    41   134     5 10000\n",
            " 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
            " 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
            " 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
            " 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
            " 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
            " 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
            " 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
            " 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
            " 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
            " 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
            " 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
            " 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
            " 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
            " 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
            " 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
            " 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
            " 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
            " 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
            " 10000 10000 10000 10000 10000 10000 10000 10000]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model(torch.tensor(np.array([x])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyApLMs5_GGj",
        "outputId": "ab7c6516-0e0d-45e2-91fe-19a0394e728a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.0201,  0.0375,  0.3536,  ...,  0.0626,  0.2839, -0.3163],\n",
              "         [-0.2946, -0.2074,  0.4911,  ...,  0.5775,  0.5217, -0.0270],\n",
              "         [-0.1077,  0.1105,  0.5981,  ..., -0.8316,  0.4529,  0.0826],\n",
              "         ...,\n",
              "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model(torch.tensor(np.array([x]))).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Yq3MqYUDZ5Z",
        "outputId": "33b47a83-9ff7-40ad-d6ac-a7af490a167c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 500, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# notice that this is currently (1, seq_len, embedding_dim) when we use this in our dataloader we'll have (batch_size, seq_len, embedding_dim) but the convolutional layers we use expect (batch_size, embedding_dim, seq_len)\n",
        "\n",
        "# Let's look at how we can create a custom layer for use in Pytorch that will change this for us\n",
        "\n",
        "class Transpose(nn.Module):\n",
        "    def __init__(self, dim0, dim1):\n",
        "        super().__init__()\n",
        "        self.dim0 = dim0\n",
        "        self.dim1 = dim1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.transpose(x,self.dim0, self.dim1)\n",
        "        return x"
      ],
      "metadata": {
        "id": "BKdB8geXGBPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a utility to help us calculate the dims of the hidden layer following our convolution layer\n",
        "\n",
        "def calc_linear_dim(max_seq_len, num_filters, kernel_size, stride, max_pool_size):\n",
        "  \"\"\"\n",
        "  Calculates the in size of a linear layer following convolutions and max pooling\n",
        "\n",
        "  Assumes no padding is used\n",
        "  \"\"\"\n",
        "  return ((max_seq_len-kernel_size)//stride + 1)//max_pool_size*num_filters"
      ],
      "metadata": {
        "id": "b1m2K4isDh5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We'll use this in combination with a CNN since now we have sequential data\n",
        "# let's try to use the embeddings we loaded earlier now\n",
        "\n",
        "# recall max_seq_len is 500 right now\n",
        "\n",
        "max_seq_len = 500\n",
        "num_filters = 64\n",
        "kernel_size = 5\n",
        "stride = 1\n",
        "max_pool_size = 20\n",
        "\n",
        "linear_size = calc_linear_dim(max_seq_len, num_filters, kernel_size, stride, max_pool_size)\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Embedding.from_pretrained(torch.from_numpy(embedding_matrix)), # by default this is frozen ie will not be updated during training\n",
        "    Transpose(1, 2),\n",
        "    nn.Conv1d(100, num_filters, kernel_size),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool1d(max_pool_size),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(linear_size, 1),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce2X0hOfCgYY",
        "outputId": "d8136de1-1a16-4165-e40c-286393320c52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Embedding(10001, 100)\n",
            "  (1): Transpose()\n",
            "  (2): Conv1d(100, 64, kernel_size=(5,), stride=(1,))\n",
            "  (3): ReLU()\n",
            "  (4): MaxPool1d(kernel_size=20, stride=20, padding=0, dilation=1, ceil_mode=False)\n",
            "  (5): Flatten(start_dim=1, end_dim=-1)\n",
            "  (6): Linear(in_features=1536, out_features=1, bias=True)\n",
            "  (7): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn, train_loss_history, valid_loss_history, valid_accuracy_history = run_training_loop(model, batch_size=32, n_epochs=15, lr=5e-3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFY7SiYLlnGo",
        "outputId": "6e3c7c81-02b2-4bec-f986-47334dc09129"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-031e17bf88a2>:45: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print('Train Epoch: %d  Average loss: %.4f' %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1  Average loss: 0.0230\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-031e17bf88a2>:66: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print('Valid set: Average loss: %.4f, Accuracy: %d/%d (%.4f)\\n' %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valid set: Average loss: 0.0222, Accuracy: 504/1000 (50.4000)\n",
            "\n",
            "Train Epoch: 2  Average loss: 0.0219\n",
            "Valid set: Average loss: 0.0220, Accuracy: 545/1000 (54.5000)\n",
            "\n",
            "Train Epoch: 3  Average loss: 0.0208\n",
            "Valid set: Average loss: 0.0211, Accuracy: 610/1000 (61.0000)\n",
            "\n",
            "Train Epoch: 4  Average loss: 0.0195\n",
            "Valid set: Average loss: 0.0201, Accuracy: 663/1000 (66.3000)\n",
            "\n",
            "Train Epoch: 5  Average loss: 0.0166\n",
            "Valid set: Average loss: 0.0188, Accuracy: 684/1000 (68.4000)\n",
            "\n",
            "Train Epoch: 6  Average loss: 0.0134\n",
            "Valid set: Average loss: 0.0185, Accuracy: 701/1000 (70.1000)\n",
            "\n",
            "Train Epoch: 7  Average loss: 0.0096\n",
            "Valid set: Average loss: 0.0195, Accuracy: 715/1000 (71.5000)\n",
            "\n",
            "Train Epoch: 8  Average loss: 0.0063\n",
            "Valid set: Average loss: 0.0238, Accuracy: 700/1000 (70.0000)\n",
            "\n",
            "Train Epoch: 9  Average loss: 0.0032\n",
            "Valid set: Average loss: 0.0211, Accuracy: 713/1000 (71.3000)\n",
            "\n",
            "Train Epoch: 10  Average loss: 0.0017\n",
            "Valid set: Average loss: 0.0222, Accuracy: 719/1000 (71.9000)\n",
            "\n",
            "Train Epoch: 11  Average loss: 0.0008\n",
            "Valid set: Average loss: 0.0249, Accuracy: 727/1000 (72.7000)\n",
            "\n",
            "Train Epoch: 12  Average loss: 0.0005\n",
            "Valid set: Average loss: 0.0247, Accuracy: 728/1000 (72.8000)\n",
            "\n",
            "Train Epoch: 13  Average loss: 0.0003\n",
            "Valid set: Average loss: 0.0254, Accuracy: 710/1000 (71.0000)\n",
            "\n",
            "Train Epoch: 14  Average loss: 0.0002\n",
            "Valid set: Average loss: 0.0254, Accuracy: 720/1000 (72.0000)\n",
            "\n",
            "Train Epoch: 15  Average loss: 0.0001\n",
            "Valid set: Average loss: 0.0270, Accuracy: 713/1000 (71.3000)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# recall max_seq_len is 500 right now\n",
        "num_filters = 64\n",
        "kernel_size = 5\n",
        "stride = 1\n",
        "max_pool_size = 20\n",
        "\n",
        "linear_size = calc_linear_dim(max_seq_len, num_filters, kernel_size, stride, max_pool_size)\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1]), # let's fit our own custom embeddings on this data of the same shape\n",
        "    Transpose(1, 2),\n",
        "    nn.Conv1d(embedding_matrix.shape[1], num_filters, kernel_size),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool1d(max_pool_size),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(linear_size, 1),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJ-YOHinrARW",
        "outputId": "f12b5fc4-9c38-4d69-9fd4-f886dc350ad0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Embedding(10001, 100)\n",
            "  (1): Transpose()\n",
            "  (2): Conv1d(100, 64, kernel_size=(5,), stride=(1,))\n",
            "  (3): ReLU()\n",
            "  (4): MaxPool1d(kernel_size=20, stride=20, padding=0, dilation=1, ceil_mode=False)\n",
            "  (5): Flatten(start_dim=1, end_dim=-1)\n",
            "  (6): Linear(in_features=1536, out_features=1, bias=True)\n",
            "  (7): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn, train_loss_history, valid_loss_history, valid_accuracy_history = run_training_loop(model, batch_size=32, n_epochs=15, lr=5e-3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxokN4tKF_oU",
        "outputId": "19dce366-6cc6-4ed8-c346-563bd6b75feb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-031e17bf88a2>:45: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print('Train Epoch: %d  Average loss: %.4f' %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1  Average loss: 0.0324\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-031e17bf88a2>:66: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print('Valid set: Average loss: %.4f, Accuracy: %d/%d (%.4f)\\n' %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valid set: Average loss: 0.0225, Accuracy: 541/1000 (54.1000)\n",
            "\n",
            "Train Epoch: 2  Average loss: 0.0172\n",
            "Valid set: Average loss: 0.0232, Accuracy: 532/1000 (53.2000)\n",
            "\n",
            "Train Epoch: 3  Average loss: 0.0080\n",
            "Valid set: Average loss: 0.0236, Accuracy: 556/1000 (55.6000)\n",
            "\n",
            "Train Epoch: 4  Average loss: 0.0018\n",
            "Valid set: Average loss: 0.0250, Accuracy: 554/1000 (55.4000)\n",
            "\n",
            "Train Epoch: 5  Average loss: 0.0005\n",
            "Valid set: Average loss: 0.0258, Accuracy: 548/1000 (54.8000)\n",
            "\n",
            "Train Epoch: 6  Average loss: 0.0002\n",
            "Valid set: Average loss: 0.0259, Accuracy: 558/1000 (55.8000)\n",
            "\n",
            "Train Epoch: 7  Average loss: 0.0001\n",
            "Valid set: Average loss: 0.0266, Accuracy: 551/1000 (55.1000)\n",
            "\n",
            "Train Epoch: 8  Average loss: 0.0001\n",
            "Valid set: Average loss: 0.0272, Accuracy: 554/1000 (55.4000)\n",
            "\n",
            "Train Epoch: 9  Average loss: 0.0001\n",
            "Valid set: Average loss: 0.0271, Accuracy: 549/1000 (54.9000)\n",
            "\n",
            "Train Epoch: 10  Average loss: 0.0001\n",
            "Valid set: Average loss: 0.0280, Accuracy: 547/1000 (54.7000)\n",
            "\n",
            "Train Epoch: 11  Average loss: 0.0001\n",
            "Valid set: Average loss: 0.0279, Accuracy: 555/1000 (55.5000)\n",
            "\n",
            "Train Epoch: 12  Average loss: 0.0000\n",
            "Valid set: Average loss: 0.0280, Accuracy: 551/1000 (55.1000)\n",
            "\n",
            "Train Epoch: 13  Average loss: 0.0000\n",
            "Valid set: Average loss: 0.0284, Accuracy: 553/1000 (55.3000)\n",
            "\n",
            "Train Epoch: 14  Average loss: 0.0000\n",
            "Valid set: Average loss: 0.0283, Accuracy: 553/1000 (55.3000)\n",
            "\n",
            "Train Epoch: 15  Average loss: 0.0000\n",
            "Valid set: Average loss: 0.0291, Accuracy: 552/1000 (55.2000)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# recall max_seq_len is 500 right now\n",
        "num_filters = 8\n",
        "kernel_size = 5\n",
        "stride = 1\n",
        "max_pool_size = 20\n",
        "embedding_dim = 32\n",
        "\n",
        "\n",
        "linear_size = calc_linear_dim(max_seq_len, num_filters, kernel_size, stride, max_pool_size)\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Embedding(embedding_matrix.shape[0], embedding_dim), # let's fit our own custom embeddings with our choice of dimensions\n",
        "    Transpose(1, 2),\n",
        "    nn.Conv1d(embedding_dim, num_filters, kernel_size),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool1d(max_pool_size),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(linear_size, 1),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DqXP7NiGM-O",
        "outputId": "1ba634ab-f6c2-4584-ef50-7f59cb1b4034"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Embedding(10001, 32)\n",
            "  (1): Transpose()\n",
            "  (2): Conv1d(32, 8, kernel_size=(5,), stride=(1,))\n",
            "  (3): ReLU()\n",
            "  (4): MaxPool1d(kernel_size=20, stride=20, padding=0, dilation=1, ceil_mode=False)\n",
            "  (5): Flatten(start_dim=1, end_dim=-1)\n",
            "  (6): Linear(in_features=192, out_features=1, bias=True)\n",
            "  (7): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn, train_loss_history, valid_loss_history, valid_accuracy_history = run_training_loop(model, batch_size=32, n_epochs=15, lr=5e-3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtxdDJTMGhzn",
        "outputId": "afe7a1ab-0581-4d8c-f0a7-7ef2261501b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-031e17bf88a2>:45: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print('Train Epoch: %d  Average loss: %.4f' %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1  Average loss: 0.0226\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-031e17bf88a2>:66: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print('Valid set: Average loss: %.4f, Accuracy: %d/%d (%.4f)\\n' %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valid set: Average loss: 0.0222, Accuracy: 519/1000 (51.9000)\n",
            "\n",
            "Train Epoch: 2  Average loss: 0.0202\n",
            "Valid set: Average loss: 0.0224, Accuracy: 529/1000 (52.9000)\n",
            "\n",
            "Train Epoch: 3  Average loss: 0.0167\n",
            "Valid set: Average loss: 0.0220, Accuracy: 553/1000 (55.3000)\n",
            "\n",
            "Train Epoch: 4  Average loss: 0.0114\n",
            "Valid set: Average loss: 0.0233, Accuracy: 568/1000 (56.8000)\n",
            "\n",
            "Train Epoch: 5  Average loss: 0.0057\n",
            "Valid set: Average loss: 0.0239, Accuracy: 580/1000 (58.0000)\n",
            "\n",
            "Train Epoch: 6  Average loss: 0.0022\n",
            "Valid set: Average loss: 0.0249, Accuracy: 573/1000 (57.3000)\n",
            "\n",
            "Train Epoch: 7  Average loss: 0.0010\n",
            "Valid set: Average loss: 0.0260, Accuracy: 579/1000 (57.9000)\n",
            "\n",
            "Train Epoch: 8  Average loss: 0.0006\n",
            "Valid set: Average loss: 0.0266, Accuracy: 573/1000 (57.3000)\n",
            "\n",
            "Train Epoch: 9  Average loss: 0.0004\n",
            "Valid set: Average loss: 0.0273, Accuracy: 580/1000 (58.0000)\n",
            "\n",
            "Train Epoch: 10  Average loss: 0.0003\n",
            "Valid set: Average loss: 0.0281, Accuracy: 576/1000 (57.6000)\n",
            "\n",
            "Train Epoch: 11  Average loss: 0.0002\n",
            "Valid set: Average loss: 0.0282, Accuracy: 580/1000 (58.0000)\n",
            "\n",
            "Train Epoch: 12  Average loss: 0.0001\n",
            "Valid set: Average loss: 0.0284, Accuracy: 575/1000 (57.5000)\n",
            "\n",
            "Train Epoch: 13  Average loss: 0.0001\n",
            "Valid set: Average loss: 0.0291, Accuracy: 579/1000 (57.9000)\n",
            "\n",
            "Train Epoch: 14  Average loss: 0.0001\n",
            "Valid set: Average loss: 0.0289, Accuracy: 577/1000 (57.7000)\n",
            "\n",
            "Train Epoch: 15  Average loss: 0.0001\n",
            "Valid set: Average loss: 0.0308, Accuracy: 578/1000 (57.8000)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}